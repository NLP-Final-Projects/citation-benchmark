{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "C4CYHeXJvygY",
        "MwY33ndowL8K",
        "0lqYT7CKwY8w",
        "OIb6oljrwsQZ",
        "a5o9EwScwyHA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eedc5afd201345229a9ee82ff8bc30ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1263bbd624f44d168e9e70d1371e8b26",
              "IPY_MODEL_e3c70a1c828441eeb26763c7dd26ef5f",
              "IPY_MODEL_cb0cf9cf297945adb1fbe1e4b92d6987"
            ],
            "layout": "IPY_MODEL_6eb11b65f29b4a5e9cb175f9916e5b12"
          }
        },
        "1263bbd624f44d168e9e70d1371e8b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd3b87406fcc496b82ecefab683efeb6",
            "placeholder": "​",
            "style": "IPY_MODEL_b4876cd4a8af46119941b763bd5738d0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e3c70a1c828441eeb26763c7dd26ef5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d3a59579ed942f7a1c47998a1eb7db3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_241504e2b1da4c9297b4ee6ed37f9fe8",
            "value": 2
          }
        },
        "cb0cf9cf297945adb1fbe1e4b92d6987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ac81e0f1bef40dab07f6c7e6086e590",
            "placeholder": "​",
            "style": "IPY_MODEL_d521c60c368546578607378ed6073b37",
            "value": " 2/2 [00:53&lt;00:00, 24.59s/it]"
          }
        },
        "6eb11b65f29b4a5e9cb175f9916e5b12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd3b87406fcc496b82ecefab683efeb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4876cd4a8af46119941b763bd5738d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d3a59579ed942f7a1c47998a1eb7db3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "241504e2b1da4c9297b4ee6ed37f9fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ac81e0f1bef40dab07f6c7e6086e590": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d521c60c368546578607378ed6073b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "qXYB7HIT1-sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **My Code**"
      ],
      "metadata": {
        "id": "C4CYHeXJvygY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z46oP12xvsqk"
      },
      "outputs": [],
      "source": [
        "!mkdir prompts\n",
        "!gdown 1o5nAmvgI0RnNhvac2gALFDlRgNcfZvsu\n",
        "!mv prompts.zip ./prompts/\n",
        "%cd prompts\n",
        "!unzip prompts.zip\n",
        "!rm prompts.zip\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir configs\n",
        "!gdown 1yQQDa0BhNb930_oBrYIJ41CksRENQvbU\n",
        "!mv configs.zip ./configs/\n",
        "%cd configs\n",
        "!unzip configs.zip\n",
        "!rm configs.zip\n",
        "%cd .."
      ],
      "metadata": {
        "id": "joI8CC8jPwJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **download_data.sh**"
      ],
      "metadata": {
        "id": "MwY33ndowL8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/datasets/princeton-nlp/ALCE-data/resolve/main/ALCE-data.tar\n",
        "!tar xvf ALCE-data.tar\n",
        "!mv ALCE-data data\n",
        "!rm ALCE-data.tar"
      ],
      "metadata": {
        "id": "5cDCOjaiwD5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **utils.py**"
      ],
      "metadata": {
        "id": "0lqYT7CKwY8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import string\n",
        "import time\n",
        "\n",
        "def normalize_answer(s):\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def remove_citations(sent):\n",
        "    return re.sub(r\"\\[\\d+\", \"\", re.sub(r\" \\[\\d+\", \"\", sent)).replace(\" |\", \"\").replace(\"]\", \"\")\n",
        "\n",
        "\n",
        "def get_max_memory():\n",
        "    \"\"\"Get the maximum memory available for the current GPU for loading models.\"\"\"\n",
        "    free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
        "    max_memory = f'{free_in_GB-6}GB'\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    max_memory = {i: max_memory for i in range(n_gpus)}\n",
        "    return max_memory\n",
        "\n",
        "\n",
        "def make_doc_prompt(doc, doc_id, doc_prompt, use_shorter=None):\n",
        "    # For doc prompt:\n",
        "    # - {ID}: doc id (starting from 1)\n",
        "    # - {T}: title\n",
        "    # - {P}: text\n",
        "    # use_shorter: None, \"summary\", or \"extraction\"\n",
        "\n",
        "    text = doc['text']\n",
        "    if use_shorter is not None:\n",
        "        text = doc[use_shorter]\n",
        "    return doc_prompt.replace(\"{T}\", doc[\"title\"]).replace(\"{P}\", text).replace(\"{ID}\", str(doc_id+1))\n",
        "\n",
        "\n",
        "def get_shorter_text(item, docs, ndoc, key):\n",
        "    doc_list = []\n",
        "    for item_id, item in enumerate(docs):\n",
        "        if key not in item:\n",
        "            if len(doc_list) == 0:\n",
        "                # If there aren't any document, at least provide one (using full text)\n",
        "                item[key] = item['text']\n",
        "                doc_list.append(item)\n",
        "            logger.warn(f\"No {key} found in document. It could be this data do not contain {key} or previous documents are not relevant. This is document {item_id}. This question will only have {len(doc_list)} documents.\")\n",
        "            break\n",
        "        if \"irrelevant\" in item[key] or \"Irrelevant\" in item[key]:\n",
        "            continue\n",
        "        doc_list.append(item)\n",
        "        if len(doc_list) >= ndoc:\n",
        "            break\n",
        "    return doc_list\n",
        "\n",
        "\n",
        "def make_demo(item, prompt, ndoc=None, doc_prompt=None, instruction=None, use_shorter=None, test=False):\n",
        "    # For demo prompt\n",
        "    # - {INST}: the instruction\n",
        "    # - {D}: the documents\n",
        "    # - {Q}: the question\n",
        "    # - {A}: the answers\n",
        "    # ndoc: number of documents to put in context\n",
        "    # use_shorter: None, \"summary\", or \"extraction\"\n",
        "\n",
        "    prompt = prompt.replace(\"{INST}\", instruction).replace(\"{Q}\", item['question'])\n",
        "    if \"{D}\" in prompt:\n",
        "        if ndoc == 0:\n",
        "            prompt = prompt.replace(\"{D}\\n\", \"\") # if there is no doc we also delete the empty line\n",
        "        else:\n",
        "            doc_list = get_shorter_text(item, item[\"docs\"], ndoc, use_shorter) if use_shorter is not None else item[\"docs\"][:ndoc]\n",
        "            text = \"\".join([make_doc_prompt(doc, doc_id, doc_prompt, use_shorter=use_shorter) for doc_id, doc in enumerate(doc_list)])\n",
        "            prompt = prompt.replace(\"{D}\", text)\n",
        "\n",
        "    if not test:\n",
        "        answer = \"\\n\" + \"\\n\".join(item[\"answer\"]) if isinstance(item[\"answer\"], list) else item[\"answer\"]\n",
        "        prompt = prompt.replace(\"{A}\", \"\").rstrip() + answer\n",
        "    else:\n",
        "        prompt = prompt.replace(\"{A}\", \"\").rstrip() # remove any space or \\n\n",
        "\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def load_model(model_name_or_path, dtype=torch.float16, int8=False, reserve_memory=10):\n",
        "    # Load a huggingface model and tokenizer\n",
        "    # dtype: torch.float16 or torch.bfloat16\n",
        "    # int8: whether to use int8 quantization\n",
        "    # reserve_memory: how much memory to reserve for the model on each gpu (in GB)\n",
        "\n",
        "    # Load the FP16 model\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "    logger.info(f\"Loading {model_name_or_path} in {dtype}...\")\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # Added !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    if int8:\n",
        "        logger.warn(\"Use LLM.int8\")\n",
        "    start_time = time.time()\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name_or_path,\n",
        "#         device_map='auto',\n",
        "        device_map=device,\n",
        "        torch_dtype=dtype,\n",
        "        max_memory=get_max_memory(),\n",
        "        load_in_8bit=int8,\n",
        "        offload_folder='offload_folder', # Added !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    )\n",
        "    logger.info(\"Finish loading in %.2f sec.\" % (time.time() - start_time))\n",
        "\n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)\n",
        "\n",
        "    # Fix OPT bos token problem in HF\n",
        "    if \"opt\" in model_name_or_path:\n",
        "        tokenizer.bos_token = \"<s>\"\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "tA1SI-q6wX1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **searcher.py**"
      ],
      "metadata": {
        "id": "OIb6oljrwsQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.functional import normalize\n",
        "\n",
        "def doc_to_text_tfidf(doc):\n",
        "    return doc['title'] + ' ' + doc['text']\n",
        "\n",
        "def doc_to_text_dense(doc):\n",
        "    return doc['title'] + '. ' + doc['text']\n",
        "\n",
        "\n",
        "class SearcherWithinDocs:\n",
        "\n",
        "    def __init__(self, docs, retriever, model=None, device=\"cuda\"):\n",
        "        self.retriever = retriever\n",
        "        self.docs = docs\n",
        "        self.device = device\n",
        "        if retriever == \"tfidf\":\n",
        "            self.tfidf = TfidfVectorizer()\n",
        "            self.tfidf_docs = self.tfidf.fit_transform([doc_to_text_tfidf(doc) for doc in docs])\n",
        "        elif \"gtr\" in retriever:\n",
        "            self.model = model\n",
        "            self.embeddings = self.model.encode([doc_to_text_dense(doc) for doc in docs], device=self.device, convert_to_numpy=False, convert_to_tensor=True, normalize_embeddings=True)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def search(self, query):\n",
        "        # Return the top-1 result doc id\n",
        "\n",
        "        if self.retriever == \"tfidf\":\n",
        "            tfidf_query = self.tfidf.transform([query])[0]\n",
        "            similarities = [cosine_similarity(tfidf_doc, tfidf_query) for tfidf_doc in self.tfidf_docs]\n",
        "            best_doc_id = np.argmax(similarities)\n",
        "            return best_doc_id\n",
        "        elif \"gtr\" in self.retriever:\n",
        "            q_embed = self.model.encode([query], device=self.device, convert_to_numpy=False, convert_to_tensor=True, normalize_embeddings=True)\n",
        "            score = torch.matmul(self.embeddings, q_embed.t()).squeeze(1).detach().cpu().numpy()\n",
        "            best_doc_id = np.argmax(score)\n",
        "            return best_doc_id\n",
        "        else:\n",
        "            raise NotImplementedError"
      ],
      "metadata": {
        "id": "TeRJ3B0-wp0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split Data**"
      ],
      "metadata": {
        "id": "UsDWOD6havmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mv ./data/asqa_eval_gtr_top100.json ./data/asqa_eval_gtr_top100_original.json"
      ],
      "metadata": {
        "id": "oD4bat313SvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "f = open('./data/asqa_eval_gtr_top100_original.json')\n",
        "data = json.load(f)\n",
        "f.close()\n",
        "\n",
        "new_data = data[:50]\n",
        "f = open('./data/asqa_eval_gtr_top100.json', \"w\")\n",
        "json.dump(new_data, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "SHdS-PCn3wrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Change Summary**"
      ],
      "metadata": {
        "id": "OGx6Hw6qbAI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('./data/asqa_eval_gtr_top100.json')\n",
        "data = json.load(f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "QTkARm6gbDNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "\n",
        "def reconstruct_sentence(sentence, percentage=0.25):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    new_sentence_words = []\n",
        "    for token in doc:\n",
        "        if token.ent_iob_ == 'B' or token.ent_iob_ == 'I':\n",
        "            if not new_sentence_words or new_sentence_words[-1] != token.ent_iob_:\n",
        "                new_sentence_words.append(token.text)\n",
        "        else:\n",
        "            if random.random() < percentage:\n",
        "                new_sentence_words.append(token.text)\n",
        "\n",
        "    return new_sentence_words"
      ],
      "metadata": {
        "id": "AUJPpqYz2LZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "hf_access_token = \"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\"\n",
        "os.environ[\"HF_ACCESS_TOKEN\"] = hf_access_token"
      ],
      "metadata": {
        "id": "oHzeO8Uu2p1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "d1_ieeJW2r_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence_with_entities(entities):\n",
        "    prompt = f\"Generate a sentence using the following entities: {', '.join(entities)}.\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_length=500,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        num_beams=5,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    generated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_sentence"
      ],
      "metadata": {
        "id": "-p_k1gF93ck6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, q in enumerate(data):\n",
        "    docs = q['docs']\n",
        "    for j, doc in tqdm(enumerate(docs)):\n",
        "        text = doc['text']\n",
        "        new_text = reconstruct_sentence(text)\n",
        "        generated_sentence = generate_sentence_with_entities(new_text)\n",
        "        data[i]['docs'][j]['text'] = generated_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "2vx6bgD2bNVI",
        "outputId": "618df0e1-ae20-4901-c39b-22fab78295ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1797: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "0it [00:18, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1ad2e4e0c5d6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mnew_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreconstruct_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mgenerated_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_sentence_with_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'docs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-59f4e5957634>\u001b[0m in \u001b[0;36mgenerate_sentence_with_entities\u001b[0;34m(entities)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     outputs = model.generate(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1952\u001b[0m             \u001b[0;31m# 14. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   1954\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unchanged original behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m                 outputs = self(\n\u001b[0m\u001b[1;32m   2915\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m                     \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1175\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    976\u001b[0m                 )\n\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    979\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# 1. Dequantize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;31m# 2. MatmulnN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequantize_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;31m# 3. Save state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('./data/asqa_eval_gtr_top100.json', \"w\")\n",
        "json.dump(data, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "EGvSl9Vd11WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **run.py**"
      ],
      "metadata": {
        "id": "a5o9EwScwyHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai safetensors"
      ],
      "metadata": {
        "id": "6nmh8OF0w0Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import openai\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "import time\n",
        "import string\n",
        "import numpy as np\n",
        "import re\n",
        "# from searcher import SearcherWithinDocs\n",
        "import yaml\n",
        "# from utils import *\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "def remove_citations(sent):\n",
        "    return re.sub(r\"\\[\\d+\", \"\", re.sub(r\" \\[\\d+\", \"\", sent)).replace(\" |\", \"\").replace(\"]\", \"\")\n",
        "\n",
        "class LLM:\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "\n",
        "        if args.openai_api:\n",
        "            import openai\n",
        "            OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "            OPENAI_ORG_ID = os.environ.get(\"OPENAI_ORG_ID\")\n",
        "            OPENAI_API_BASE = os.environ.get(\"OPENAI_API_BASE\")\n",
        "\n",
        "            if args.azure:\n",
        "                openai.api_key = OPENAI_API_KEY\n",
        "                openai.api_base = OPENAI_API_BASE\n",
        "                openai.api_type = 'azure'\n",
        "                openai.api_version = '2023-05-15'\n",
        "            else:\n",
        "                openai.api_key = OPENAI_API_KEY\n",
        "                openai.organization = OPENAI_ORG_ID\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", fast_tokenizer=False) # TODO: For ChatGPT we should use a different one\n",
        "            # To keep track of how much the API costs\n",
        "            self.prompt_tokens = 0\n",
        "            self.completion_tokens = 0\n",
        "        else:\n",
        "            self.model, self.tokenizer = load_model(args.model)\n",
        "\n",
        "        self.prompt_exceed_max_length = 0\n",
        "        self.fewer_than_50 = 0\n",
        "        self.azure_filter_fail = 0\n",
        "\n",
        "\n",
        "    def generate(self, prompt, max_tokens, stop=None):\n",
        "        args = self.args\n",
        "        if max_tokens <= 0:\n",
        "            self.prompt_exceed_max_length += 1\n",
        "            logger.warning(\"Prompt exceeds max length and return an empty string as answer. If this happens too many times, it is suggested to make the prompt shorter\")\n",
        "            return \"\"\n",
        "        if max_tokens < 50:\n",
        "            self.fewer_than_50 += 1\n",
        "            logger.warning(\"The model can at most generate < 50 tokens. If this happens too many times, it is suggested to make the prompt shorter\")\n",
        "\n",
        "        if args.openai_api:\n",
        "            use_chat_api = (\"turbo\" in args.model and not args.azure) or (\"gpt-4\" in args.model and args.azure)\n",
        "            if use_chat_api:\n",
        "                # For chat API, we need to convert text prompts to chat prompts\n",
        "                prompt = [\n",
        "                    {'role': 'system', 'content': \"You are a helpful assistant that answers the following questions with proper citations.\"},\n",
        "                    {'role': 'user', 'content': prompt}\n",
        "                ]\n",
        "            if args.azure:\n",
        "                deploy_name = args.model\n",
        "\n",
        "            if use_chat_api:\n",
        "                is_ok = False\n",
        "                retry_count = 0\n",
        "                while not is_ok:\n",
        "                    retry_count += 1\n",
        "                    try:\n",
        "                        response = openai.ChatCompletion.create(\n",
        "                            engine=deploy_name if args.azure else None,\n",
        "                            model=args.model,\n",
        "                            messages=prompt,\n",
        "                            temperature=args.temperature,\n",
        "                            max_tokens=max_tokens,\n",
        "                            stop=stop,\n",
        "                            top_p=args.top_p,\n",
        "                        )\n",
        "                        is_ok = True\n",
        "                    except Exception as error:\n",
        "                        if retry_count <= 5:\n",
        "                            logger.warning(f\"OpenAI API retry for {retry_count} times ({error})\")\n",
        "                            continue\n",
        "                        print(error)\n",
        "                        import pdb; pdb.set_trace()\n",
        "                self.prompt_tokens += response['usage']['prompt_tokens']\n",
        "                self.completion_tokens += response['usage']['completion_tokens']\n",
        "                return response['choices'][0]['message']['content']\n",
        "            else:\n",
        "                is_ok = False\n",
        "                retry_count = 0\n",
        "                while not is_ok:\n",
        "                    retry_count += 1\n",
        "                    try:\n",
        "                        response = openai.Completion.create(\n",
        "                            engine=deploy_name if args.azure else None,\n",
        "                            model=args.model,\n",
        "                            prompt=prompt,\n",
        "                            temperature=args.temperature,\n",
        "                            max_tokens=max_tokens,\n",
        "                            top_p=args.top_p,\n",
        "                            stop=[\"\\n\", \"\\n\\n\"] + (stop if stop is not None else [])\n",
        "                        )\n",
        "                        is_ok = True\n",
        "                    except Exception as error:\n",
        "                        if retry_count <= 5:\n",
        "                            logger.warning(f\"OpenAI API retry for {retry_count} times ({error})\")\n",
        "                            if \"triggering Azure OpenAI’s content management policy\" in str(error):\n",
        "                                # filtered by Azure\n",
        "                                self.azure_filter_fail += 1\n",
        "                                return \"\"\n",
        "                            continue\n",
        "                        print(error)\n",
        "                        import pdb; pdb.set_trace()\n",
        "                self.prompt_tokens += response['usage']['prompt_tokens']\n",
        "                self.completion_tokens += response['usage']['completion_tokens']\n",
        "                return response['choices'][0]['text']\n",
        "        else:\n",
        "            inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(self.model.device)\n",
        "            stop = [] if stop is None else stop\n",
        "            stop = list(set(stop + [\"\\n\", \"Ċ\", \"ĊĊ\", \"<0x0A>\"])) # In Llama \\n is <0x0A>; In OPT \\n is Ċ\n",
        "            stop_token_ids = list(set([self.tokenizer._convert_token_to_id(stop_token) for stop_token in stop] + [self.model.config.eos_token_id]))\n",
        "            if \"llama\" in args.model.lower():\n",
        "                stop_token_ids.remove(self.tokenizer.unk_token_id)\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                do_sample=True, temperature=args.temperature, top_p=args.top_p,\n",
        "                max_new_tokens=max_tokens,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=stop_token_ids\n",
        "            )\n",
        "            generation = self.tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
        "            return generation\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--config\", type=str, default=None, help=\"Path to the config file\")\n",
        "\n",
        "    # Prompt file is a json file that contains the following fields:\n",
        "    # - instruction: the instruction, which will appear at the beginning of each demo and the test example\n",
        "    # - demo_sep: the separator between each demo, for example, \"\\n\\n\\n\"\n",
        "    # - demo_prompt: the prompt for the demo, for example, \"Instruction: {INST}\\n\\nQuestion: {Q}\\n\\n{D}\\nAnswer: {A}\"\n",
        "    #     - {INST}: the instruction\n",
        "    #     - {D}: the documents\n",
        "    #     - {Q}: the question\n",
        "    #     - {A}: the answers\n",
        "    # - doc_prompt, the prompt for each document, for example, \"Document [{ID}](Title: {T}): {P}\", where\n",
        "    #     - {ID}: the document id, staring from 1\n",
        "    #     - {T}: the document title\n",
        "    #     - {P}: the document text\n",
        "    # - demos: a list of demo examples, each of which should have\n",
        "    #     - question: the question\n",
        "    #     - docs: the documents (\"title\" and \"text\")\n",
        "    #     - answer: the answer to show in the demo. If it is a list, they will be concatenated by \"\\n\". This is useful when the answer includes interactive components.\n",
        "    # Note that this python file will sample `--shot` demos from the prompt file given the random seed `--seed`\n",
        "    parser.add_argument(\"--prompt_file\", type=str, help=\"Path to the prompt file\")\n",
        "\n",
        "    # Evaluation file is a json file that contains a list of item, each of which contains\n",
        "    # - question: the question\n",
        "    # - answer: the answer\n",
        "    # - docs: the documents, each of which contains \"title\", \"text\"\n",
        "    parser.add_argument(\"--eval_file\", type=str, help=\"Path to the eval file\")\n",
        "    parser.add_argument(\"--quick_test\", type=int, default=None, help=\"Quickly test a few examples\")\n",
        "\n",
        "    # ICL setting\n",
        "    parser.add_argument(\"--ndoc\", type=int, help=\"Number of documents\")\n",
        "    parser.add_argument(\"--shot\", type=int, help=\"Number of ICL demonstrations\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Seed for the random number generator\")\n",
        "    parser.add_argument(\"--no_doc_in_demo\", type=bool, default=False, help=\"Whether to remove the documents in the demos\")\n",
        "    parser.add_argument(\"--fewer_doc_in_demo\", type=bool, default=False, help=\"Whether to use fewer documents in the demos\")\n",
        "    parser.add_argument(\"--ndoc_in_demo\", type=int, default=None, help=\"When using --fewer_doc_in_demo, use this to designate how many docs in demo\")\n",
        "\n",
        "    # Model and name\n",
        "    parser.add_argument(\"--dataset_name\", type=str, help=\"Name of the dataset (for saving)\")\n",
        "    parser.add_argument(\"--tag\", type=str, help=\"Tag of run (for saving)\")\n",
        "    parser.add_argument(\"--model\", type=str, help=\"Model to use\")\n",
        "    parser.add_argument(\"--openai_api\", type=bool, default=False, help=\"Whether to use OpenAI API\")\n",
        "    parser.add_argument(\"--azure\", action=\"store_true\", default=False, help=\"Azure openai API\")\n",
        "\n",
        "    # Decoding\n",
        "    parser.add_argument(\"--temperature\", type=float, default=0.5, help=\"Temperature for decoding\")\n",
        "    parser.add_argument(\"--top_p\", type=float, default=1.0, help=\"Nucleus sampling top-p\")\n",
        "    parser.add_argument(\"--max_new_tokens\", type=int, default=300, help=\"Max number of new tokens to generate in one step\")\n",
        "    parser.add_argument(\"--max_length\", type=int, default=2048, help=\"Max length the model can take. Should set properly wrt the model to avoid position overflow.\")\n",
        "    parser.add_argument(\"--num_samples\", type=int, default=1, help=\"Sample multiple answers.\")\n",
        "\n",
        "    # Use summarization/extraction of the documents\n",
        "    parser.add_argument(\"--use_shorter\", type=str, default=None, help=\"Whether to use summary data or extraction data for documents. Option: None, `summary`, `extraction`\")\n",
        "\n",
        "    # Interactive\n",
        "    parser.add_argument(\"--interactive\", type=bool, default=False, help=\"Whether to run in interactive mode\")\n",
        "    parser.add_argument(\"--interactive_query\", type=str, default=None, help=\"The query to use in interactive mode, either `doc_id` (corresponding to interact in paper) or `search` (corresponding to inlinesearch in paper).\")\n",
        "    parser.add_argument(\"--retriever\", type=str, default=None, help=\"When using interactive search mode, which retriever to use. Options: `tfidf`, `gtr-t5-large`\")\n",
        "    parser.add_argument(\"--retriever_device\", type=str, default=\"cuda\", help=\"Where to put the dense retriever if using. Options: `cuda`, `cpu`\")\n",
        "    parser.add_argument(\"--retrieve_in_all_docs\", type=bool, default=False, help=\"Retrieve in all documents instead of just top ndoc\")\n",
        "    parser.add_argument(\"--max_turn\", type=int, default=10, help=\"Max number of all actions\")\n",
        "    parser.add_argument(\"--max_doc_show\", type=int, default=3, help=\"Max number of documents to show at one time.\")\n",
        "    parser.add_argument(\"--force_cite_show\", type=bool, default=False, help=\"Force citing the documents that are shown to the model\")\n",
        "\n",
        "\n",
        "    # Load config\n",
        "    args = parser.parse_args()\n",
        "    config = yaml.safe_load(open(args.config)) if args.config is not None else {}\n",
        "    parser.set_defaults(**config)\n",
        "    args = parser.parse_args()\n",
        "    for k in args.__dict__:\n",
        "        print(f\"{k}: {args.__dict__[k]}\")\n",
        "\n",
        "    if \"turbo\" in args.model:\n",
        "        # ChatGPT has a longer max length\n",
        "        args.max_length = 4096\n",
        "\n",
        "    if \"16k\" in args.model:\n",
        "        args.max_length = 16384\n",
        "    elif \"32k\" in args.model:\n",
        "        args.max_length = 32768\n",
        "    elif \"turbo\" in args.model:\n",
        "        args.max_length = 4096\n",
        "    elif \"gpt-4\" in args.model:\n",
        "        args.max_length = 8192\n",
        "    elif \"llama-2\" in args.model.lower() or \"llama2\" in args.model.lower():\n",
        "        args.max_length = 4096\n",
        "\n",
        "\n",
        "    logger.info(f\"Set the model max length to {args.max_length} (if not correct, check the code)\")\n",
        "\n",
        "\n",
        "    # Load the model or setup the API\n",
        "    llm = LLM(args)\n",
        "\n",
        "    # Generate prompts\n",
        "    np.random.seed(args.seed)\n",
        "\n",
        "    # Load data\n",
        "    prompt_data = json.load(open(args.prompt_file))\n",
        "    eval_data = json.load(open(args.eval_file))\n",
        "\n",
        "    # Generate the demonstration part\n",
        "    head_prompt = \"\"\n",
        "    train_ids = np.random.choice(len(prompt_data[\"demos\"]), args.shot, replace=False)\n",
        "    for train_id in train_ids:\n",
        "        train_item = prompt_data[\"demos\"][train_id]\n",
        "        ndoc = args.ndoc\n",
        "        if args.no_doc_in_demo:\n",
        "            ndoc = 0\n",
        "        elif args.fewer_doc_in_demo:\n",
        "            assert args.ndoc_in_demo is not None\n",
        "            ndoc = args.ndoc_in_demo\n",
        "        head_prompt += make_demo(\n",
        "            train_item, prompt=prompt_data[\"demo_prompt\"], ndoc=ndoc, doc_prompt=prompt_data[\"doc_prompt\"],\n",
        "            instruction=prompt_data[\"instruction\"], use_shorter=args.use_shorter\n",
        "        )\n",
        "        head_prompt += prompt_data[\"demo_sep\"]\n",
        "\n",
        "    # Sample quick test\n",
        "    if args.quick_test is not None:\n",
        "        eval_ids = np.random.choice(len(eval_data), args.quick_test, replace=False)\n",
        "        eval_data = [eval_data[int(idx)] for idx in eval_ids]\n",
        "\n",
        "    logger.info(\"Generating prompts...\")\n",
        "    incomplete_doc_list = 0 # For some questions there might be fewer than ndoc documents\n",
        "    for idx, eval_item in enumerate(tqdm(eval_data)):\n",
        "        eval_data[idx]['prompt'] = head_prompt + make_demo(\n",
        "            eval_item, prompt=prompt_data[\"demo_prompt\"], ndoc=args.ndoc, doc_prompt=prompt_data[\"doc_prompt\"],\n",
        "            instruction=prompt_data[\"instruction\"], use_shorter=args.use_shorter,\n",
        "            test=True\n",
        "        )\n",
        "        doc_list = get_shorter_text(eval_item, eval_item[\"docs\"], args.ndoc, args.use_shorter) if args.use_shorter is not None else eval_item[\"docs\"][:args.ndoc]\n",
        "        if not args.retrieve_in_all_docs:\n",
        "            # If --retrieve_in_all_docs, we keep the original docs and do not trim them by ndoc\n",
        "            # Otherwise, take the new docs (truncated by ndoc and filtered if using summary/extraction)\n",
        "            eval_data[idx]['docs'] = doc_list\n",
        "        if len(doc_list) < args.ndoc:\n",
        "            incomplete_doc_list += 1\n",
        "    logger.info(\"Done.\")\n",
        "    if incomplete_doc_list > 0:\n",
        "        logger.warning(f\"There are {incomplete_doc_list} questions that have incomplete document list (may due to a lot of them are filtered out by summary/extraction).\")\n",
        "\n",
        "    # Load retriever for interactive search\n",
        "    if args.interactive and args.interactive_query == \"search\" and \"gtr\" in args.retriever:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        gtr_model = SentenceTransformer(f'sentence-transformers/{args.retriever}', device=args.retriever_device)\n",
        "        from searcher import SearcherWithinDocs\n",
        "\n",
        "    for idx, item in enumerate(tqdm(eval_data)):\n",
        "        prompt = item['prompt']\n",
        "        prompt_len = len(llm.tokenizer.tokenize(prompt))\n",
        "\n",
        "        if idx == 0:\n",
        "            print(prompt)\n",
        "\n",
        "        output_array = []\n",
        "        for _ in range(args.num_samples):\n",
        "            if args.interactive:\n",
        "                print(\"============ Interactive =============\")\n",
        "                output_answer = \"\"\n",
        "                doc_list = item['docs']\n",
        "\n",
        "                interactive_prompt = prompt.rstrip() + \"\\n\" # Start a new line\n",
        "                inline_doc = \"\"\n",
        "                num_turn = 0\n",
        "\n",
        "                doc_history = []\n",
        "                while True:\n",
        "                    # For each action, it should end at the new line\n",
        "                    # Three possible actions\n",
        "                    # - Check: Document [1][2][3] / search query\n",
        "                    # - Output: output\n",
        "                    # - End\n",
        "                    num_turn += 1\n",
        "                    new_prompt = interactive_prompt + inline_doc\n",
        "                    new_prompt_len = len(llm.tokenizer.tokenize(new_prompt))\n",
        "\n",
        "                    if idx == 0:\n",
        "                        print(f\"-------------- Step {num_turn} prompt --------------\")\n",
        "                        print(new_prompt)\n",
        "                        print(\"-----------------------------\")\n",
        "\n",
        "                    output = llm.generate(new_prompt, min(args.max_new_tokens, args.max_length-new_prompt_len), stop=[\"\\n\", \"\\n\\n\"])\n",
        "\n",
        "                    if len(inline_doc) > 0:\n",
        "                        output = \"Output: \" + output # \"Output: \" was included in inline_doc\n",
        "                    inline_doc = \"\" # Delete inline_doc after use\n",
        "                    interactive_prompt += output + \"\\n\"\n",
        "                    logger.info(f\"Model output: \\\"{output}\\\"\")\n",
        "\n",
        "                    if output.strip().lower()[:3] == \"end\":\n",
        "                        # Model decides to end the generation\n",
        "                        break\n",
        "                    elif \"sorry\" in output.lower() and (\"relevant document\" in output.lower() or \"relevant information\" in output.lower()) or \"none of the documents\" in output.lower():\n",
        "                        # Instruction-tuned model may abstain from answer the question\n",
        "                        break\n",
        "                    elif output.strip().lower()[:5] == \"check\" or output.strip().lower()[:6] == \"search\":\n",
        "                        # Checkout or search documents\n",
        "                        if args.interactive_query == \"search\":\n",
        "                            query = output.replace(\"Search:\", \"\").replace(\"search:\", \"\").strip()\n",
        "                            if len(doc_list) == 0:\n",
        "                                show_doc_ids = []\n",
        "                            else:\n",
        "                                searcher = SearcherWithinDocs(doc_list, args.retriever, model=gtr_model, device=args.retriever_device)\n",
        "                                show_doc_ids = [int(searcher.search(query))]\n",
        "                        elif args.interactive_query == \"doc_id\":\n",
        "                            show_doc_ids = [int(r[1:])-1 for r in re.findall(r\"\\[\\d+\", output)] # In text citation id starts from 1\n",
        "                            show_doc_ids = [doc_id for doc_id in show_doc_ids if doc_id < len(doc_list) and doc_id >= 0]\n",
        "                            show_doc_ids = show_doc_ids[:args.max_doc_show] # Avoiding showing too many documents\n",
        "                        else:\n",
        "                            raise NotImplementedError\n",
        "\n",
        "                        inline_doc = \"\".join([make_doc_prompt(doc_list[doc_id], doc_id, prompt_data[\"doc_prompt\"]) for doc_id in show_doc_ids])\n",
        "                        inline_doc += \"Output:\" # Force the model to generate output in the next step\n",
        "                        doc_history.append(show_doc_ids)\n",
        "                    elif output.strip().lower()[:6] == \"output\":\n",
        "                        output = output.strip().replace(\"Output:\", \"\").strip()\n",
        "                        if args.force_cite_show:\n",
        "                            output = remove_citations(output)\n",
        "                            if len(doc_history) == 0:\n",
        "                                logger.warn(\"No doc history??\")\n",
        "                            else:\n",
        "                                # Just cite whatever documents the model has seen in the last step\n",
        "                                if \"qampari\" in args.eval_file:\n",
        "                                    output = \", \".join([\"\".join([f\"[{doc+1}]\" for doc in doc_history[-1]]) + \" \" + entity.strip() for entity in output.rstrip().rstrip(\",\").split(\",\")]) + \", \"\n",
        "                                else:\n",
        "                                    output = \" \".join([\"\".join([f\"[{doc+1}]\" for doc in doc_history[-1]]) + \" \" + o for o in sent_tokenize(output)]) + \".\"\n",
        "                        output_answer += \" \" + output\n",
        "                    else:\n",
        "                        # Sometimes model starts to output random things.\n",
        "                        break\n",
        "\n",
        "                    if num_turn >= args.max_turn:\n",
        "                        logger.warning(\"Reach maximum number of turns. Terminate now.\")\n",
        "                        break\n",
        "\n",
        "                if \"qampari\" in args.eval_file:\n",
        "                    output_answer = output_answer.rstrip().rstrip(\",\")\n",
        "                output_array.append(output_answer)\n",
        "                item['prompt'] = interactive_prompt\n",
        "                item['doc_history'] = doc_history\n",
        "            else:\n",
        "                output_array.append(llm.generate(prompt, min(args.max_new_tokens, args.max_length-prompt_len)))\n",
        "                item['prompt'] = prompt\n",
        "\n",
        "            output_array[-1] = output_array[-1].replace(\"<|im_end|>\", \"\").rstrip()\n",
        "            if output_array[-1].endswith(\"End.\"):\n",
        "                output_array[-1] = output_array[-1][:-len(\"End.\")]\n",
        "\n",
        "            logger.info(f\"Prompt length={prompt_len}\")\n",
        "            logger.info(f\"Question: {item['question']}\")\n",
        "            logger.info(f\"Gold answer: {item['answer']}\")\n",
        "            logger.info(f\"Final model output: {output_array[-1]}\")\n",
        "\n",
        "        item['output'] = output_array if len(output_array) > 1 else output_array[0]\n",
        "\n",
        "    logger.info(f\"#Cases when prompts exceed max length: {llm.prompt_exceed_max_length}\")\n",
        "    logger.info(f\"#Cases when max new tokens < 50: {llm.fewer_than_50}\")\n",
        "\n",
        "    # Save the result\n",
        "    model_name = args.model\n",
        "    if \"/\" in model_name:\n",
        "        model_name = model_name.split(\"/\")[-1]\n",
        "    name = f\"{args.dataset_name}-{model_name}-{args.tag}-shot{args.shot}-ndoc{args.ndoc}-{args.seed}\"\n",
        "    if args.azure:\n",
        "        name += \"-azure\"\n",
        "    if args.quick_test is not None:\n",
        "        name += f\"-quick_test{args.quick_test}\"\n",
        "    if args.no_doc_in_demo:\n",
        "        name += \"-no_doc_in_demo\"\n",
        "    if args.fewer_doc_in_demo:\n",
        "        name += f\"-{args.ndoc_in_demo}_doc_in_demo\"\n",
        "    if args.num_samples > 1:\n",
        "        name += f\"-sample{args.num_samples}\"\n",
        "    if args.force_cite_show:\n",
        "        name += f\"-forceciteshow\"\n",
        "\n",
        "\n",
        "    eval_data = {\n",
        "        \"args\": args.__dict__,\n",
        "        \"data\": eval_data,\n",
        "    }\n",
        "    if args.openai_api:\n",
        "        logger.info(f\"Token used: prompt {llm.prompt_tokens}; completion {llm.completion_tokens}\")\n",
        "        if \"turbo\" in args.model:\n",
        "            p_price, c_price = 0.0015, 0.002\n",
        "            if \"16k\" in args.model:\n",
        "                p_price, c_price = 0.003, 0.004\n",
        "        elif \"gpt4\" in args.model or \"gpt-4\" in args.model:\n",
        "            p_price, c_price = 0.03, 0.06\n",
        "            if \"32k\" in args.model:\n",
        "                p_price, c_price = 0.06, 0.12\n",
        "        else:\n",
        "            logger.warn(\"Cannot find model price\")\n",
        "            p_price, c_price = 0, 0\n",
        "\n",
        "        eval_data[\"total_cost\"] = llm.prompt_tokens / 1000 * p_price + llm.completion_tokens / 1000 * c_price\n",
        "\n",
        "        logger.info(f\"Unit price (Oct 16, 2023, prompt/completion): {p_price}/{c_price}\")\n",
        "        logger.info(f\"Total cost: %.1f\" % (eval_data[\"total_cost\"]))\n",
        "\n",
        "        if args.azure:\n",
        "            eval_data[\"azure_filter_fail\"] = llm.azure_filter_fail\n",
        "\n",
        "    if not os.path.exists(\"result\"):\n",
        "        os.makedirs(\"result\")\n",
        "    json.dump(eval_data, open(\"result/\" + name + \".json\", \"w\"), indent=4)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sys.argv = \"run.py --config configs/asqa_opt-6.7b_shot1_ndoc3_gtr_default.yaml\".split()\n",
        "#     sys.argv = \"run.py --config configs/asqa_alpaca-7b_shot1_ndoc3_gtr_default.yaml\".split()\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "eedc5afd201345229a9ee82ff8bc30ce",
            "1263bbd624f44d168e9e70d1371e8b26",
            "e3c70a1c828441eeb26763c7dd26ef5f",
            "cb0cf9cf297945adb1fbe1e4b92d6987",
            "6eb11b65f29b4a5e9cb175f9916e5b12",
            "cd3b87406fcc496b82ecefab683efeb6",
            "b4876cd4a8af46119941b763bd5738d0",
            "7d3a59579ed942f7a1c47998a1eb7db3",
            "241504e2b1da4c9297b4ee6ed37f9fe8",
            "1ac81e0f1bef40dab07f6c7e6086e590",
            "d521c60c368546578607378ed6073b37"
          ]
        },
        "id": "2qH4ZOarxFJ4",
        "outputId": "11cf79f9-049d-4d4f-df59-a18d6ee3afa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Set the model max length to 2048 (if not correct, check the code)\n",
            "INFO:__main__:Loading facebook/opt-6.7b in torch.float16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config: configs/asqa_opt-6.7b_shot1_ndoc3_gtr_default.yaml\n",
            "prompt_file: prompts/asqa_default.json\n",
            "eval_file: data/asqa_eval_gtr_top100.json\n",
            "quick_test: None\n",
            "ndoc: 3\n",
            "shot: 1\n",
            "seed: 42\n",
            "no_doc_in_demo: False\n",
            "fewer_doc_in_demo: False\n",
            "ndoc_in_demo: None\n",
            "dataset_name: asqa\n",
            "tag: gtr\n",
            "model: facebook/opt-6.7b\n",
            "openai_api: False\n",
            "azure: False\n",
            "temperature: 1.0\n",
            "top_p: 0.95\n",
            "max_new_tokens: 300\n",
            "max_length: 2048\n",
            "num_samples: 1\n",
            "use_shorter: None\n",
            "interactive: False\n",
            "interactive_query: None\n",
            "retriever: None\n",
            "retriever_device: cuda\n",
            "retrieve_in_all_docs: False\n",
            "max_turn: 10\n",
            "max_doc_show: 3\n",
            "force_cite_show: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eedc5afd201345229a9ee82ff8bc30ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Finish loading in 57.49 sec.\n",
            "INFO:__main__:Generating prompts...\n",
            "100%|██████████| 50/50 [00:00<00:00, 18779.91it/s]\n",
            "INFO:__main__:Done.\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction: Write an accurate, engaging, and concise answer for the given question using only the provided search results (some of which might be irrelevant) and cite them properly. Use an unbiased and journalistic tone. Always cite for any factual claim. When citing several search results, use [1][2][3]. Cite at least one document and at most three documents in each sentence. If multiple documents support the sentence, only cite a minimum sufficient subset of the documents.\n",
            "\n",
            "Question: When did the us break away from england?\n",
            "\n",
            "Document [1](Title: United States withdrawal from Saudi Arabia): United States withdrawal from Saudi Arabia Beginning during Operation Desert Shield in August 1990, while preparing for the Gulf War, the United States sent a large troop contingent to Saudi Arabia. After the war, remnant troops, primarily U.S. Air Force personnel, augmented by a smaller number of coordinating and training personnel from the U.S. Navy, U.S. Army and U.S. Marine Corps remained in Saudi Arabia under the aegis of Joint Task Force Southwest Asia (JTF-SWA), as part of Operation Southern Watch (OSW). The United Kingdom and France also maintained a small contingent of Royal Air Force and French Air Force\n",
            "Document [2](Title: Decolonization of the Americas): and France has fully \"integrated\" most of its former colonies as fully constituent \"departments\" of France. The United States of America declared independence from Great Britain on July 2, 1776 (although the event is now commemorated on July 4, the date when the Declaration of Independence was officially adopted by Congress), in so doing becoming the first independent, foreign-recognized nation in the Americas and the first European colonial entity to break from its mother country. Britain formally acknowledged American independence in 1783 after its defeat in the American Revolutionary War. Although initially occupying only the land east of the Mississippi\n",
            "Document [3](Title: American Revolution): second British army at Yorktown in the fall of 1781, effectively ending the war. The Treaty of Paris was signed September 3, 1783, formally ending the conflict and confirming the new nation's complete separation from the British Empire. The United States took possession of nearly all the territory east of the Mississippi River and south of the Great Lakes, with the British retaining control of Canada and Spain taking Florida. Among the significant results of the revolution was the creation of the United States Constitution, establishing a relatively strong federal national government that included an executive, a national judiciary, and\n",
            "\n",
            "Answer:The United States took the first step towards gaining independence from Great Britain when it declared independence from Great Britain on July 2, 1776 (although the event is now commemorated on July 4, 1776, the date when the Declaration of Independence was officially adopted by Congress) [2]. The Treaty of Paris was later signed on September 3, 1783, formally separating the United States from the British Empire [3].\n",
            "\n",
            "\n",
            "Instruction: Write an accurate, engaging, and concise answer for the given question using only the provided search results (some of which might be irrelevant) and cite them properly. Use an unbiased and journalistic tone. Always cite for any factual claim. When citing several search results, use [1][2][3]. Cite at least one document and at most three documents in each sentence. If multiple documents support the sentence, only cite a minimum sufficient subset of the documents.\n",
            "\n",
            "Question: Who has the highest goals in world football?\n",
            "\n",
            "Document [1](Title: Argentina–Brazil football rivalry): \"Football Player of the Century\", by IFFHS International Federation of Football History and Statistics, 1999, \"South America Football Player of the Century\", by IFFHS International Federation of Football History and Statistics. Pelé's 1281 goals are recognized by FIFA as the highest total achieved by a professional footballer, although the Soccer Statistic Foundation (rssf) recognizes only 767 goals in official mode, occupying the third place after Josef Bican (805) and Romario (772). For his part, Maradona has been named the best soccer player in World Cup history both by The Times and FourFourTwo, publication that also rewarded him as the \"Best\n",
            "Document [2](Title: Godfrey Chitalu): have beaten Gerd Müller's record of 85 goals in a year, the Football Association of Zambia claimed that the world record actually pertained to Godfrey Chitalu who had scored 116 goals (possibly 117) during the 1972 calendar year and 107 during the 1972 season. The difference of goals is due to first 9 goals being scored before the season officially started. The Football Association of Zambia presented the evidence to FIFA but a spokesperson responded that they would ratify neither Lionel Messi's nor Chitalu's records as they do not keep statistical track of domestic competitions. Nonetheless, it could constitute the\n",
            "Document [3](Title: Godfrey Chitalu): highest official tally claimed by a national football association. Chitalu made his international debut on 29 June 1968 in a friendly match against Uganda in Lusaka which Zambia won 2–1. He scored his first goal in a 2–2 draw against the same team five days later. Chitalu played a prominent role during the World Cup qualification matches against Sudan with Zambia being eliminated on a strange rule which was peculiar to Africa and favoured the team that won the second leg. Despite the aggregate score being tied at 6–6 after Zambia won the first leg 4–2 and lost the return\n",
            "\n",
            "Answer:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Prompt length=1130\n",
            "INFO:__main__:Question: Who has the highest goals in world football?\n",
            "INFO:__main__:Gold answer: The players with the highest all-time goals and highest men's and women's international football goals differ. The player with the highest all-time men's football goals is Josef Bican, who in 2020 was recognized by FIFA, the international governing body of football, as the record scorer with an estimated 805 goals. Christine Sinclair has the highest goals in women's international football with 187 and is the all-time leader for international goals scored for men or women. Cristiano Ronaldo and Ali Daei are currently tied for leading goalscorer in the history of men's international football with 109.\n",
            "INFO:__main__:Final model output: Lionel Messi is the greatest footballer of all time according to the IFFHS Football Player of the Century competition in which Messi scored a total of 1281 goals (1281, being an unlucky 13), while Pele is in second place after scoring 1267 goals for Santos. [1]. The record is the highest total in any competition and is regarded as the highest score in soccer by the Soccer Statistic Foundation (rssf). The Football Association of Zambia claimed that the world record actually pertained to Godfrey Chitalu who scored 116 goals (possibly 117) during the 1972 calendar year and 107 during the 1972 season. The difference of goals is due to first 9 goals being scored before the season officially started. The Football Association of Zambia presented the evidence to FIFA but a spokesperson responded that they would ratify neither Lionel Messi's nor Chitalu's records as they do not keep statistical track of domestic competitions. Nonetheless, it could constitute the\n",
            "  2%|▏         | 1/50 [00:17<14:18, 17.53s/it]INFO:__main__:Prompt length=1114\n",
            "INFO:__main__:Question: Who is the original artist of sound of silence?\n",
            "INFO:__main__:Gold answer: There are several songs with the title \"Sound of Silence\". Sounds of Silence is the second studio album by Simon & Garfunkel, released on January 17, 1966. The album's title is a slight modification of the title of the duo's first major hit, \"The Sound of Silence\", which was recorded in March 1964 and originally was released as \"The Sounds of Silence\". Another \"Sound of Silence\" is a song performed by Australian recording artist Dami Im, and is best known as Australia's entry at the Eurovision Song Contest 2016.\n",
            "INFO:__main__:Final model output: The Sound of Silence \"The Sound of Silence\", originally \"The Sounds of Silence\", is a song by the American music duo Simon & Garfunkel. The song was written by Paul Simon over a period of several months in 1963 and 1964. A studio audition led to the duo signing a record deal with Columbia Records, and the song was recorded in March 1964 at Columbia Studios in New York City for inclusion on their debut album, \"Wednesday Morning, 3 A.M.\". Released in October 1964, the album was a commercial failure and led to the duo breaking apart, with Paul Simon\n",
            "  4%|▍         | 2/50 [00:26<10:12, 12.75s/it]INFO:__main__:Prompt length=1130\n",
            "INFO:__main__:Question: When was the first apple i phone made?\n",
            "INFO:__main__:Gold answer: There were several Apple iPhones, including the Apple iPhone beta and the Apple iPhone. The first Apple iPhone for beta testing, the Apple iPhone beta, was made in 2004. The iPhone beta was made to test the device and its functions but was never released to the public. The first Apple iPhone 1 was made and the first Apple iPhone was released on June 29, 2007.\n",
            "INFO:__main__:Final model output: The Apple I was released in 1976, and was the first computer ever sold. The Apple II was the first computer with the \"home computer\" label. The Apple III was the first computer that included a mass storage device (hard disk). The Apple IIe and IIc were the first computers which could use more than one hard drive. The Apple IIGS was the first computer to support the IBM PC (and later Windows). The Apple Macintosh was first released in 1984. The original Macintosh used the Motorola 68000. It was followed up by Macintosh Plus and Macintosh 512K. The Macintosh Portable was the first Macintosh laptop computer. The first model of iPod was released in 2001. The original iPod was released in 2005. The first iPod Touch and iPhone were released in 2007. The first iPhone 3G was announced in January 2009. The first iPhone 3GS was announced in October 2009. The first iPhone 4S was announced in September 2010.\n",
            "  6%|▌         | 3/50 [00:41<10:39, 13.60s/it]INFO:__main__:Prompt length=1130\n",
            "INFO:__main__:Question: Who played the weasley brothers in harry potter?\n",
            "INFO:__main__:Gold answer: The Weasely family is a family of wizards including Molly and Arthur Weasely as well as their seven children. Richard Fish appeared as Bill briefly in the film adaptation of Harry Potter and the Prisoner of Azkaban. Domhnall Gleeson, the son of actor Brendan Gleeson (Alastor Moody in the series), played Bill Weasley in Harry Potter and the Deathly Hallows. Chris Rankin played Percy Weasely and Fred and George Weasely were played by identical twins James Phelps and Oliver Phelps. Rupert Grint played Ron Weasely.\n",
            "INFO:__main__:Final model output: The twins went to an open audition in 2000, and, after about six auditions, they were cast as Fred and George Weasley in the film \"Harry Potter and the Philosopher's Stone\" [2]. The two began filming for the Harry Potter film series on 2 September 2000, and finished shooting the final film, \"Harry Potter and the Deathly Hallows – Part 2\" in July 2009 [3].\n",
            "  8%|▊         | 4/50 [00:48<08:29, 11.07s/it]INFO:__main__:Prompt length=1115\n",
            "INFO:__main__:Question: How many state parks are there in virginia?\n",
            "INFO:__main__:Gold answer: Virginia opened its entire state park system on June 15, 1936 as a six-park system. The six original state parks were Seashore State Park, now First Landing State Park, Westmoreland State Park, Staunton River State Park, Douthat State Park, Fairy Stone State Park, and Hungry Mother State Park. Natural Bridge State Park officially opened on September 24, 2016, making this 38 parks in VA. Today, the park system now oversees 43 parks.\n",
            "INFO:__main__:Final model output: Carson, who was appointed Secretary of the Interior in 1935 by President Franklin Delano Roosevelt, was not popular with Virginia politicians, in particular those in the South. They opposed the creation of parks in their states, as well as the parkways (highways running through parks) which the National Park Service was attempting to create, especially in their own districts. While the state parks and state forests were largely unsuccessful in their first few years, these lands remained important to the state's citizens. These were the initial goals of the department of the environment: \"...protection, conservation, recreation, preservation of the natural and man-made heritage of the State, and study of the history, development, environment, geography and natural resources of Virginia for the benefit and enjoyment of present and future generations.\"\n",
            " 10%|█         | 5/50 [01:00<08:35, 11.47s/it]INFO:__main__:Prompt length=1143\n",
            "INFO:__main__:Question: Who performed at the champions league final 2018?\n",
            "INFO:__main__:Gold answer: Real Madrid and Liverpool were the teams that performed in the 2018 UEFA Champion League Final. Two goals from man of the match Gareth Bale proved the difference in a 3–1 win for Real Madrid Entertainers, Dua Lipa and Sean Paul performed for the opening ceremony, and the anthem was performed by 2Cellos, Luka Sulic and Stjephan Hauser. \n",
            "INFO:__main__:Final model output: The 2016 UEFA Champions League Final was held two days prior, on 26 May 2016, at the Mapei Stadium – Città del Tricolore [1]. American singer Alicia Keys performed in the opening ceremony prior to the match, the first time it has featured a live music performance. Italian tenor Andrea Bocelli performed the UEFA Champions League Anthem. The 2016 UEFA Women's Champions League Final was held two days earlier, on 24 May 2016, at the Valeriy Lobanovskyi Dynamo Stadium between Wolfsburg and Lyon, Lyon emerging victorious 4–1. This was also the last time that the host city for the men's Champions League final was also automatically assigned the Women's Champions League final. This was also the first time since 2008 that the host cities of the men's and women's Champions League finals were both outside the United Kingdom, since Milan and Rome each hosted the final of the previous season respectively. The annual UEFA Champions Festival was held between 24–27 May 2018 at the Kiev city centre. In late May, a special vocal version of the UEFA Champions League anthem titled \"UEFA Champions League\" with lyrics in other languages, changing over to the host country's language for the chorus, were performed by\n",
            " 12%|█▏        | 6/50 [01:19<10:17, 14.04s/it]INFO:__main__:Prompt length=1133\n",
            "INFO:__main__:Question: Who killed the man in thelma and louise?\n",
            "INFO:__main__:Gold answer: Thelma & Louise is a 1991 American female buddy road crime film directed by Ridley Scott and written by Callie Khouri. It stars Geena Davis as Thelma and Susan Sarandon as Louise, two friends who embark on a road trip that ends up in unforeseen circumstances. Louise, played by Susan Sarandon, killed a man in the movie in a fit of rage.\n",
            "INFO:__main__:Final model output: Marcus (Robert Redford) [1] kills One-Round (Clive Owen) in order to rescue his and Louis' [2] life after finding out that Louis is a government agent who is trying to kill Marcus because he's a witness to the murder of Mr. Wrench (Paul Newman) [3].\n",
            " 14%|█▍        | 7/50 [01:25<08:03, 11.24s/it]INFO:__main__:Prompt length=1181\n",
            "INFO:__main__:Question: Who plays charlie on it's always sunny?\n",
            "INFO:__main__:Gold answer: Charlie Day plays the fictional character Charlie Kelly in the FX series It's Always Sunny in Philadelphia. Day is an American actor, screenwriter, producer, and comedian who is best known for playing Charlie Kelly on the sitcom, for which he was nominated for a Critics' Choice Television Award and a Satellite Award in 2011.\n",
            "INFO:__main__:Final model output:  On April 1, 2016, the FX network released a trailer announcing that \"It's Always Sunny in Philadelphia\" would return to the air on April 1, 2016, for its 14th season. The episode, titled \"Charlie Work,\" was written and directed by Rob McElhenney. The title of the episode refers to Charlie (Charlie Day) having written a screenplay for an epic movie that, according to Charlie's character, has no plot. The episode premiered on FXX on April 1, 2016. In it, Charlie is set to direct the premiere of his movie. During the shoot, he finds himself in a battle with Danny Devito's character, Frank Reynolds, who is threatening to leave the show. The crew later discovers that Charlie's script has an actual plot. On January 31, 2016, FX released the official poster for the new season of \"It's Always Sunny in Philadelphia.\" On February 7, 2016, FXX released a teaser for \"Charlie Work\" and released a full trailer online on February 17, 2016.\n",
            " 16%|█▌        | 8/50 [01:41<08:59, 12.84s/it]INFO:__main__:Prompt length=1103\n",
            "INFO:__main__:Question: How many times have the lakers won the finals?\n",
            "INFO:__main__:Gold answer: The Los Angeles Lakers are an American professional basketball team based in Los Angeles. The Lakers compete in the National Basketball Association, NBA, as a member of the league's Western Conference Pacific Division. The Lakers are one of the most successful teams in the history of the NBA, since 2010 the Lakers won the championship finals 16 times, they won their 17th in 2020. \n",
            "INFO:__main__:Final model output: In the history of the NBA, the lakers have won a total of 16 titles. This includes the 6 titles they have won since 2008. The lakers have also won 11 titles in the 1960's, 8 in the 1970's and 3 in the 1980's. The lakers have made the playoffs for all 20 of their seasons with Kareem in his prime. However, in the 80's when Kobe and Shaq were on the team, the lakers did not make the playoffs. The lakers have faced off with the celtics 11 times in NBA finals. The celtics have won 11 of these matches. The two teams have faced off again in the 2010 NBA finals, but the lakers again were victorious.\n",
            " 18%|█▊        | 9/50 [01:53<08:32, 12.49s/it]INFO:__main__:Prompt length=1122\n",
            "INFO:__main__:Question: How many states in india are under congress?\n",
            "INFO:__main__:Gold answer: While India has 28 states, there are additionally 8 union territories, for a total of 36 entities, of which 7 are under congress. Of the 28 states of India, which have their own governments, 5 of the states are under congress. Two of the eight union territories, which are federal territories governed by the Union Government of India, are under congress.\n",
            "INFO:__main__:Final model output: Congress is in power in the states of Punjab, Chhattisgarh, Rajasthan and Madhya Pradesh where the party has majority support. In Karnataka and Puducherry it shares power with alliance partner Janata Dal (Secular) and Dravida Munnetra Kazhagam. In Chhattisgarh it has a tie up with Telangana Rashtra Samithi and Janta Party.\n",
            " 20%|██        | 10/50 [02:00<07:13, 10.85s/it]INFO:__main__:Prompt length=1173\n",
            "INFO:__main__:Question: Who is fruma sarah in fiddler on the roof?\n",
            "INFO:__main__:Gold answer: There are several versions of Fiddler on the Roof. The original Broadway production of the show, which opened in 1964, had the first musical theatre run in history to surpass 3,000 performances. The character of Fruma Sarah, a ghostly depiction of the late wife of Lazar Wolf, is played by Carol Sawyer Yussel. In the 1971 film, Fiddler on the Roof, which theatrically released on November 3, 1971, by United Artists to critical and commercial success, Ruth Madoc played Fruma Sarah. In the 2015-2016 Broadway Revival of Fiddler on the Roof, American singer and actress Jessica Vosk played Fruma Sarah.\n",
            "INFO:__main__:Final model output:  The actress Ruth Madoc played Fruma Sarah in the film version of \"Fiddler on the Roof\" (1971) and the musical was later staged on Broadway in 1972 and won the Tony Award for best musical [1]. In addition, she appeared as the mother of Fanny Brice (played by Barbra Streisand) in \"Funny Girl\" (1968) [2].\n",
            " 22%|██▏       | 11/50 [02:07<06:15,  9.64s/it]INFO:__main__:Prompt length=1146\n",
            "INFO:__main__:Question: When did toronto host the mlb all-star game?\n",
            "INFO:__main__:Gold answer: The 1991 Major League Baseball (MLB) All-Star Game was the 62nd playing of the midsummer classic between the all-stars of the American League (AL) and National League (NL), the two leagues comprising Major League Baseball. The venue for each All-Star Game is chosen by an MLB selection committee. Toronto hosted the 1991 MLB All-Star Game on July 9, 1991.\n",
            "INFO:__main__:Final model output: The 2007 Major League Baseball All-Star Game was the 78th midsummer exhibition between the all-stars of the American League (AL) and the National League (NL), the two leagues comprising Major League Baseball. The game was held on July 10, 2007, at AT&T Park, the home of the NL's San Francisco Giants. It marked the third time that the Giants hosted the All Star Game since moving to San Francisco for the 1958 season. The 1961 and 1984 All Star Games were played at the Giants former home Candlestick Park, and the fourth overall in San Francisco after the 1972 All Star Game, and the 2001 All Star Game.\n",
            " 24%|██▍       | 12/50 [02:17<06:14,  9.84s/it]INFO:__main__:Prompt length=1100\n",
            "INFO:__main__:Question: What kind of car in to catch a thief?\n",
            "INFO:__main__:Gold answer: The make of the car in the 1953 film To Catch a Thief was Rootes Group, and its model was the 1953 Sunbeam Alpine Mk I. The car driven by Grace Kelly, who stars in the film, was a metallic blue 1953 Sunbeam Alpine Mk I. The Sunbeam Alpine is a two-seater sports drophead coupé that was produced by the Rootes Group, a British automobile manufacturer, from 1953 to 1955 and from 1959 to 1968.\n",
            "INFO:__main__:Final model output: In the case of a car that was set up as a trap to catch thieves, the car thieves would probably use a variety of methods to disable it. In order to use a \"kill switch\" that would render the car useless, the thief would have to know where to find and remove the switch. If the police are using a bait car that has been left unlocked with the keys in the ignition, the key has a very short life. In some cases the thief would be able to locate and remove a kill switch before being arrested. Once a car thief realizes he is caught and his vehicle has been disabled, it will be an easy target and easy to catch.\n",
            " 26%|██▌       | 13/50 [02:28<06:14, 10.12s/it]INFO:__main__:Prompt length=1136\n",
            "INFO:__main__:Question: When did the last season of jersey shore air?\n",
            "INFO:__main__:Gold answer: Jersey Shore is an American reality television series that ran on MTV from December 3, 2009, to December 20, 2012, in the United States. Season 4 premiered August 4, 2011 and ended on October 20, 2011. Season 5 started on January 5, 2012 and ended on March 15, 2012. Season 6 started on October 4, 2012 and ended on December 20, 2012.\n",
            "INFO:__main__:Final model output: The last season of Jersey shore aired from February 13, 2011 to March 22, 2011. It is the fifth season of the show. The seventh season premiered on January 27, 2014, following an extended break.\n",
            " 28%|██▊       | 14/50 [02:32<04:57,  8.25s/it]INFO:__main__:Prompt length=1144\n",
            "INFO:__main__:Question: What season of greys anatomy was the plane crash?\n",
            "INFO:__main__:Gold answer: There were several plane crashes on the American television medical drama Grey's Anatomy. In \"Flight\", the season final of Season 8, 6 doctors from Seattle Grace Mercy West Hospital are victims of an aviation accident, and Dr. Lexie Grey ultimately dies. In \"One Flight Down\" in Season 11, there was a plane crash in downtown Seattle that brought memories of a previous plane crash.\n",
            "INFO:__main__:Final model output: The season opener of Greys Anatomy, “Flight,” was released on September 22, 2013, as part of its eighth season. On March 9, 2014, it was announced that it would be included in season 8, episode 20 of the series. One flight down was aired on April 16, 2015, as part of season 14, episode 17. The episode features a plane crash in Seattle bringing patients to Grey Sloan Memorial and old memories of the season 8’s tragic plane crash that claimed the lives of Mark Sloan and Lexie Grey back to Meredith Grey, Arizona Robbins and Owen Hunt. The episode’s premiere aired on Thursday night on ABC in the United States, where it drew in 15 million viewers, breaking the record for highest-rated premiere, while breaking the 10 p.m. timeslot’s record in total viewers. The episode has also received a positive response from the viewers. The episode currently has a rating of 64% at Rotten Tomatoes, and an average rating of 6.78 out of 10 at IMDb.\n",
            " 30%|███       | 15/50 [02:49<06:19, 10.84s/it]INFO:__main__:Prompt length=1142\n",
            "INFO:__main__:Question: Number of branches of oriental bank of commerce in india?\n",
            "INFO:__main__:Gold answer: In August 2004, Oriental Bank of Commerce in India united with Global Trust Bank, which was a leading private sector bank, the acquisition brought with it 103 branches, which brought OBC's branch total to 1092. According to a March 2018-2019 annual report, it has 2390 branches. On April 1st, 2020, Oriental Bank of Commerce, and United Bank of India merged with Punjab National Bank making it as the second-largest public sector bank in India and brings the total amount of branches to 11,437.\n",
            "INFO:__main__:Final model output:  Oriental Bank of Commerce (OBC) was the seventh largest public sector bank (PSB) in India at the end of March 2010. It offered a wide range of banking products and services such as deposit accounts, loans, debit cards, credit cards (with tie-ups with American Express and Citibank), Internet banking facilities and the Internet banking portal www.boricash.in and other online banking services [1]. The bank had 1,993 branches (both urban and rural) as on 31 March 2010, making it the largest branch network in the country [1]. Its total business as of the end of March 2010 was 2,095.03 billion Indian rupees (approximately US$35.15 billion or approximately 6.43 trillion Indian rupees based on the exchange rate of approximately 1 INR = 42.5 US$). On 14 August 2004, Oriental Bank of Commerce (OBC) amalgamated Global Trust Bank (GTB). GTB was a leading private sector bank in India that was associated with various financial discrepancies leading to a moratorium being imposed by RBI shortly before it merged into OBC. The acquisition brought with it 103 branches, which increased OBC's branch total to 1092. The Chairpersons (CMD) of the bank were as under:\n",
            " 32%|███▏      | 16/50 [03:09<07:43, 13.64s/it]INFO:__main__:Prompt length=1118\n",
            "INFO:__main__:Question: When did the rams go to st louis?\n",
            "INFO:__main__:Gold answer: The Los Angeles Rams are a professional American football team that plays and competes in the National Football League (NFL). The Rams franchise was founded in 1936 as the Cleveland Rams in the short-lived second American Football League before joining the NFL the next year. In 1946, the franchise moved to Los Angeles. The Rams franchise remained in the metro area until 1994, when they moved to St. Louis, and were known as the St. Louis Rams from 1995 to 2015. The Rams franchise returned to Los Angeles in 2016. Their first game in St. Louis against the New Orleans Saints was played on September 10, 1995 with the Rams winning 17-13.\n",
            "INFO:__main__:Final model output: The St. Louis Rams (also known as the St. Louis Cardinals) moved from Los Angeles to St. Louis, Missouri in 1995 and later to Los Angeles in 2016. Since the Rams began playing in St. Louis in 1996, they have played their home games in the Edward Jones Dome. They have won two Super Bowls (XXXIV in 2004, XXXVI in 2002) and have gone to the playoffs 11 times. [1] The Rams first played a game at the Edward Jones Dome on October 22, 1996, a 31-10 loss to the San Francisco 49ers. In May 1997, the dome, which is a new building with seating for 60,000, was opened [2]. On February 9, 1999, the Rams hosted their first Super Bowl. They lost to the Tennessee Titans 33-14, which meant that St. Louis would not host an NFL Super Bowl for the rest of the decade. On January 7, 1999, they again played in Super Bowl XXXIV,\n",
            " 34%|███▍      | 17/50 [03:24<07:48, 14.20s/it]INFO:__main__:Prompt length=1203\n",
            "INFO:__main__:Question: When did the voortrekkers arrive in south africa?\n",
            "INFO:__main__:Gold answer: The Voortrekkers' exploratory treks arrived in South Africa in February 1835, and the first wave of Voortrekkers arrived in South Africa in 1836.\n",
            "INFO:__main__:Final model output: The Voortrekkers arrived in South Africa on November 1, 1899 [1], under the leadership of Colonel Anton van Raalte, who was the leader of the first group of Afrikaans settlers to arrive. They settled on the north side of the Orange River in the present Eastern Cape [3]. The first groups of settlers arrived at Van Diemen's Land (now known as Tasmania), with Colonel Anton van Raalte and five families arriving at Hobart, a town in north-eastern Tasmania, on 6 April 1900 [3].The Voortrekkers arrived in South Africa on November 1, 1899 [1], under the leadership of Colonel Anton van Raalte, who was the leader of the first group of Afrikaans settlers to arrive. They settled on the north side of the Orange River in the present Eastern Cape [3]. The first groups of settlers arrived at Van Diemen's Land (now known as Tasmania), with Colonel Anton van Raalte and five families arriving at Hobart, a town in north-eastern Tasmania, on 6 April 1900 [3]. This was the first Voortrekker settlement in the country. The first group of Boer trekkers in South Africa consisted of six families, who arrived at the north side of the Orange River on 1 November 1899 [1]. The settlement was named Paardeberg, the Afrikaans for 'deed in the ground', and was in existence by 1900 [1]. The settlement was\n",
            " 36%|███▌      | 18/50 [03:48<09:00, 16.90s/it]INFO:__main__:Prompt length=1169\n",
            "INFO:__main__:Question: Who plays patrick in 10 things i hate about you?\n",
            "INFO:__main__:Gold answer: The romantic comedy film 10 Things I Hate About You was released in 1999 and features Heath Ledger as the character Patrick. A TV show sitcom also called 10 Things I Hate About You was released in 2009 based on the 1999 film and features Ethan Peck as Patrick.\n",
            "INFO:__main__:Final model output: \n",
            " 38%|███▊      | 19/50 [03:49<06:15, 12.12s/it]INFO:__main__:Prompt length=1121\n",
            "INFO:__main__:Question: Microsoft live movie maker is an example of free?\n",
            "INFO:__main__:Gold answer: Windows Movie Maker, known as Windows Live Movie Maker for the 2009 and 2011 releases, is a discontinued video editing software program by Microsoft. It is an example of freely licensed software, known as freeware.\n",
            "INFO:__main__:Final model output: Microsoft live movie maker is an example of a free product.\n",
            " 40%|████      | 20/50 [03:50<04:30,  9.01s/it]INFO:__main__:Prompt length=1158\n",
            "INFO:__main__:Question: Who is the chief minister of m. p?\n",
            "INFO:__main__:Gold answer: The Chief Minister of Madhya Pradesh is the chief executive of the Indian state of Madhya Pradesh(MP). Uma Bharti (born 3 May 1959) is an Indian politician and former Chief Minister of Madhya Pradesh. She was the 15th Chief Minister of MP. Babulal Gaur Yadav (2 June 1929 – 21 August 2019) was an Indian politician from Bharatiya Janata Party (BJP) who served as the 16th Chief Minister of Madhya Pradesh. Shivraj Singh Chouhan (born 5 March 1959), often referred to as Mamaji[1] (meaning: Maternal Uncle in English) is an Indian politician and member of the Bharatiya Janata Party. He is the 17th and current Chief Minister of Madhya Pradesh.\n",
            "INFO:__main__:Final model output: Amarinder Singh Captain Amarinder Singh (born 11 March 1942) is an Indian politician, who is currently the 26th Chief Minister of Punjab. An elected Member of the Legislative Assembly from Patiala, he was also the president of Punjab Pradesh Congress Committee, the state division of the Indian National Congress. He has also previously served as the Chief Minister of Punjab from 2002-2007. His father was the last Maharaja of the princely state of Patiala. He also served in the Indian Army from 1963 to 1966. In 1980, he won a seat in the Lok Sabha for the first time. He was elected again from Patiala in the 1980s and 1990s and has served as its Member of Parliament for over two decades.\n",
            " 42%|████▏     | 21/50 [04:02<04:48,  9.94s/it]INFO:__main__:Prompt length=1160\n",
            "INFO:__main__:Question: Who sings the song for stuck in the middle?\n",
            "INFO:__main__:Gold answer: There is a TV show named Stuck in the Middle and several songs with \"Stuck in the Middle\" in the title. The 1973 song \"Stuck in the Middle with You\" was sung by the band Stealers Wheel with Gerry Rafferty singing lead vocals and Joe Egan singing harmony. The 2004 song \"Stuck in the Middle\" was sung by English girl group Clea. The theme song for the Disney Channel TV show Stuck in the Middle was sung by Sonus.\n",
            "INFO:__main__:Final model output: Gerry Rafferty and Joe Egan \"Stuck in the Middle with You\" (sometimes known as \"Stuck in the Middle\") is a song written by Scottish musicians Gerry Rafferty and Joe Egan and originally performed by their band Stealers Wheel. The band performed the song on the BBC's \"Top of the Pops\" in May 1973, and the song charted at number 8 in the UK Singles Chart. It also became an international hit, reaching number 6 in the US \"Billboard\" Hot 100. \"Stuck in the Middle\" was released on Stealers Wheel's 1972 eponymous debut album. Gerry Rafferty provided the lead vocals, while Joe Egan sang backing vocals and added the tambourine. The duo continued performing this song on \"Top of the Pops\" in\n",
            " 44%|████▍     | 22/50 [04:16<05:04, 10.89s/it]INFO:__main__:Prompt length=1106\n",
            "INFO:__main__:Question: How many grammy awards does whitney houston have?\n",
            "INFO:__main__:Gold answer: Whitney Houston has been honored with numerous awards and accolades recognizing her worldwide success through the music and movie industries, including 2 Emmy Awards, 8 Grammy Awards, 14 World Music Awards, 15 Guinness World Records, 16 Billboard Music Awards and 22 American Music Awards. Her 8 Grammy Awards include 6 competitive Grammy Awards and 2 Grammy Hall of Fame Awards.\n",
            "INFO:__main__:Final model output: She has won six Grammy Awards and been nominated 21 times.\n",
            " 46%|████▌     | 23/50 [04:17<03:41,  8.19s/it]INFO:__main__:Prompt length=1158\n",
            "INFO:__main__:Question: When was crude oil first discovered in nigeria?\n",
            "INFO:__main__:Gold answer: Nigeria is the largest oil and gas producer in Africa. Crude oil from the Niger delta basin comes in two types: light, and comparatively heavy. Oil was discovered in non-commercial quantities at Akata, near Eket in 1953. Shell-BP in the pursuit of commercially available petroleum found oil in Oloibiri, Nigeria in 1956.\n",
            "INFO:__main__:Final model output:  The oil industry in Nigeria, a former colony of Britain, began its production in 1956, in the Niger Delta oilfields, when crude oil was first discovered by Shell-BP [2]. Crude oil was discovered in 1956 in Oloibiri and was the world’s first oil to be found. The discovery paved the way for Nigeria to become one of the world’s biggest producers of oil [3]. As the population of Nigeria grew, the need for oil and petroleum products also grew and Shell-BP continued to search for oil and to extend its operations. By the early sixties, the oil industry in Nigeria was in great financial difficulties and was heavily in debt [1]. In December 1960, the Nigeria Petroleum Development Company was established with the mandate to raise money for expansion of the oil industry in Nigeria [2]. This company was later renamed the Nigerian Petroleum Development Company (NPDC) and later renamed the Nigerian National Petroleum Corporation (NNPC) after the country won independence [3]. __________________________________________________________________________________________________________\n",
            " 48%|████▊     | 24/50 [04:34<04:35, 10.60s/it]INFO:__main__:Prompt length=1148\n",
            "INFO:__main__:Question: When was the first fast and furious film made?\n",
            "INFO:__main__:Gold answer: The first film of the Fast & Furious franchise, 2001's The Fast and the Furious, was filmed in 2000 and released in 2001. Universal Studios greenlit the film in 2000. The film was shot in various locations within Los Angeles and parts of southern California from July to October 2000 and released in North America on June 22, 2001. The main films of the franchise are collectively known as The Fast Saga.\n",
            "INFO:__main__:Final model output:  The Fast and the Furious (2001 film) The Fast and the Furious is a 2001 action film directed by Rob Cohen, produced by Neal H. Moritz, and written by Gary Scott Thompson and David Ayer. It is the first installment in \"The Fast and the Furious\" franchise. The film follows Brian O'Conner (Paul Walker) a professional street racer, who is sent undercover by the CIA (the fictional Counter Narcotics Unit) to stop a drug kingpin named Dom Toretto (Vin Diesel). The film had two sequels: The Fast and the Furious 2 (2003), and The Fast and the Furious: Tokyo Drift (2006). The film was followed by a sequel television series entitled Fast & Furious which aired on NBC, from September 3 to December 24, 2006.\n",
            " 50%|█████     | 25/50 [04:46<04:38, 11.13s/it]INFO:__main__:Prompt length=1148\n",
            "INFO:__main__:Question: Who sang the song i'm coming out?\n",
            "INFO:__main__:Gold answer: \"I'm Coming Out\" is a song recorded by American singer Diana Ross. It was written and produced by Chic members Bernard Edwards and Nile Rodgers, and released on August 22, 1980, as the second single from Ross' self-titled tenth album Diana (1980). In 2003, American singer Amerie covered \"I'm Coming Out\" for the soundtrack to the film \"Maid in Manhattan\". Her version was released as a single in select European countries and Australia.\n",
            "INFO:__main__:Final model output: Ross sang the song titled \"I'm Coming Out\" [1] as the third single of her self-titled tenth album \"Diana\" [2] (1980). \"I'm Coming Out\" was the final song Chic (Bernard Edwards and Nile Rodgers) ever wrote for the album. It is not on the original LP, but as a bonus on the album's soundtrack. The video for the song was directed by Larry Clark and is known for its controversial nature [3]. Although Ross is not featured in the music video, her voice can be heard in the background throughout. It is considered by many to be a song about a coming out as a lesbian [1]. According to Nile Rodgers, the song is about \"a kind of coming out in the world of the '60s. [It's] about a woman who decides, not so much consciously to be a lesbian... It's about the fact that she feels good and comfortable being herself and being with another woman\". It was also about the new sense of acceptance, love and freedom [3]. According to Bernard Edwards, it was about \"being your true self, no matter what the circumstances might be\". The song, also known as \"The Outing Song\" can be heard during the credits of the film \"Made\" when two men are about to come out of the closet. I'm Coming Out \"I'm Coming Out\" is a song recorded by American singer Diana Ross. It was written and produced by\n",
            " 52%|█████▏    | 26/50 [05:09<05:52, 14.70s/it]INFO:__main__:Prompt length=1127\n",
            "INFO:__main__:Question: When is episode 113 of dragon ball super coming out?\n",
            "INFO:__main__:Gold answer: Dragon Ball Super is a Japanese anime television series produced by Toei Animation that began airing on July 5, 2015 on Fuji TV. The original airdate of Episode 113 was October 29, 2017. The American airdate for Episode 113 was June 1, 2019.\n",
            "INFO:__main__:Final model output: The Dragon Ball Super anime series (or \"Dragon Ball Super: Tournament of Power\") started its broadcast on April 5, 2018. The English dub of the Dragon Ball Super anime premiered on Toonami on January 21, 2017. The original Japanese Dragon Ball Super series ended on March 25, 2018 with the conclusion of the \"Universe Survival Saga\". The first official English dubbed episode of Dragon Ball Super premiered in the United States on August 13, 2018. In 2017, Dragon Ball Super: The Movie, titled Dragon Ball Super: Broly was released in Japan on December 15, 2017. The sequel, Dragon Ball Super: Broly the Legendary Super Saiyan, was released in Japan on January 12, 2018. This is the first and so far only film in the Dragon Ball franchise to be released on a limited theatrical release. The second film, Dragon Ball Super: Broly the Wrath of Black\n",
            " 54%|█████▍    | 27/50 [05:23<05:32, 14.47s/it]INFO:__main__:Prompt length=1181\n",
            "INFO:__main__:Question: Who wrote the book of 1 and 2 thessalonians?\n",
            "INFO:__main__:Gold answer: The First Epistle to the Thessalonians, commonly referred to as First Thessalonians or 1 Thessalonians, is a Pauline epistle of the New Testament of the Christian Bible that is attributed to Paul the Apostle. The Second Epistle to the Thessalonians, commonly referred to as Second Thessalonians or 2 Thessalonians is a book from the New Testament of the Christian Bible is traditionally attributed to Paul the Apostle, with Timothy as a co-author. Many modern scholars agree with New Testament Scholar Ehrman that 2 Thessalonians was not written by Paul but by an associate or disciple after his death.\n",
            "INFO:__main__:Final model output: The Second Epistle to the Thessalonians, often referred to as Second Thessalonians (US) or Two Thessalonians (UK) (and written 2 Thessalonians) is a book from the New Testament of the Christian Bible. It is traditionally attributed to Paul the Apostle, as it begins, \"Paul, Silvanus, and Timothy, To the church of the Thessalonians\" and ends, \"I, Paul, write this greeting in my own hand, which is the distinguishing mark in all my letters.\" Modern biblical scholarship is divided on whether Paul was the author or not; many scholars question its authenticity based on what they contend are weak or contradictory references to earlier churches or writings of Paul in the text. The problem is due to the composite nature of \"First Thessalonians\" (Murphy-O'Connor is only one of many scholars who argue that the current text of \"Second Thessalonians\" is the product of merging two or more authentic letters of Paul). Once the text of this interpolated letter is removed and the two letters compared, Murphy-O'Connor asserts that this objection is \"drastically weakened\", and concludes, \"The arguments against the authenticity of 2 Thessalonians are so weak that it is preferable to accept the traditional ascription of the letter to Paul.\" Those who believe Paul was the author of \"Second Thessalonians\" also note the following additional facts about the letter. First\n",
            " 56%|█████▌    | 28/50 [05:46<06:12, 16.94s/it]INFO:__main__:Prompt length=1142\n",
            "INFO:__main__:Question: When is fortnite battle royale being released on android?\n",
            "INFO:__main__:Gold answer: Fortnite Battle Royale is a free-to-play battle royale video game developed and published by Epic Games. Since its initial release on September 26, 2017, the game has been ported to numerous devices, including iOS, Android, and Nintendo Switch. The Android beta version of Fortnite was released on August 9, 2018 with a time-exclusivity for selected Samsung mobile devices. On August 13, 2018, the beta version opened up to non-Samsung devices. On October 11, 2018, Fortnite opened to all Android users, no invite required.\n",
            "INFO:__main__:Final model output: Fortnite Battle Royale was released as a beta version of the game on August 9, 2018 with a time-exclusivity for selected Samsung mobile devices until August 12, 2018 [1]. On August 13, 2018, Epic began sending invites for the Android version to registered users for non-Samsung devices, and by October 11, 2018, the Android client was made available to all without an invite. Epic distributed the Android \"Fortnite Battle Royale\" app directly from its website rather than through the Play Store, a decision that was due to a policy disagreement with Google[2]. In March 2018, Epic announced it was making \"Fortnite Battle Royale\" for Android and iOS mobile devices. The iOS version was released first, and was expected to be followed by the Android version by mid-2018. The beta version for iOS devices launched on March 15, 2018, and opened to all players on April 2, 2018. Epic Games stated that it was not possible to release the Android version with the iOS version simultaneously, and declined to provide a concrete release date for it, because the developers wanted to spend a few months making sure that the game would run without any issue on an array of different devices.\n",
            " 58%|█████▊    | 29/50 [06:04<06:06, 17.46s/it]INFO:__main__:Prompt length=1128\n",
            "INFO:__main__:Question: How many medals did australia win in the 2000 olympics?\n",
            "INFO:__main__:Gold answer: Australia was the host nation for the 2000 Summer Olympics in Sydney. That summer, Australia won a total of 58 medals. 16 of those 58 medals were gold, 25 were silver, and 17 were bronze. \n",
            "INFO:__main__:Final model output: The host nation, Australia, won four medals, including gold, silver, and bronze medals, in swimming (Francis Gailey, an Australian until recently incorrectly classed as American, won a silver and bronze). The United States took the most golds with 93, including gold medals in wrestling (Marlon McAlister), boxing (Barry Michael O'Brien), and wrestling (Shawn Green).\n",
            " 60%|██████    | 30/50 [06:12<04:47, 14.38s/it]INFO:__main__:Prompt length=1187\n",
            "INFO:__main__:Question: Who is elected as the vice president of india?\n",
            "INFO:__main__:Gold answer: Who is elected as the vice president of India depends on the year. The vice president of India is Venkaiah Naidu. He was elected in the August 5, 2017 election, defeating UPA's candidate Gopalkrishna Gandhi. Mohammad Hamid Ansari was elected to the office on August 11, 2007, and on August 7, 2012, while Bhairon Singh Shekhawat was on August 19, 2002.\n",
            "INFO:__main__:Final model output:  The Vice-President of India (in the official English language title) is the second highest office in the land (behind that of the President of India) and the first elected position. The election is usually held every 5 years and must be decided before the term of the current vice-president has finished.\n",
            " 62%|██████▏   | 31/50 [06:17<03:41, 11.65s/it]INFO:__main__:Prompt length=1097\n",
            "INFO:__main__:Question: Who was england's prime minister during ww1?\n",
            "INFO:__main__:Gold answer: At the start of World War I, England's Prime Minister was Herbert Henry Asquith, the 1st Earl of Oxford and Asquith. Generally known as H. H. Asquith, he was a British statesman and Liberal politician who served as Prime Minister of the United Kingdom from 1908 to 1916. At the end of World War I, David Lloyd George, the 1st Earl Lloyd-George of Dwyfor, was Prime Minister. He was a Welsh statesman and Liberal Party politician who served as Prime Minister of the United Kingdom from 1916 to 1922.\n",
            "INFO:__main__:Final model output:  The United Kingdom was led by David Lloyd George from 1916 until his death in 1922.\n",
            " 64%|██████▍   | 32/50 [06:19<02:38,  8.78s/it]INFO:__main__:Prompt length=1179\n",
            "INFO:__main__:Question: Who plays snowman in smokey and the bandit?\n",
            "INFO:__main__:Gold answer: Smokey and the Bandit is a 1977 American road action comedy film starring Burt Reynolds, Sally Field, Jackie Gleason, Jerry Reed, Pat McCormick, Paul Williams and Mike Henry. The directorial debut of stuntman Hal Needham, the film follows Bo \"Bandit\" Darville, played by Reynolds, and Cledus \"Snowman\" Snow played by Jerry Reed, two bootleggers attempting to illegally transport 400 cases of Coors beer from Texarkana to Atlanta. There were three parts to this film with Reed staring as \"Snowman\" for all of them. \n",
            "INFO:__main__:Final model output:  The Bandit and Snowman's friendship develops over the first three movies. In Smokey and the Bandit Part 3, Bandit is forced to leave his partner with his wife for two weeks while Bandit stays in Atlanta to compete in the Southern Classic race.\n",
            " 66%|██████▌   | 33/50 [06:24<02:08,  7.57s/it]INFO:__main__:Prompt length=1133\n",
            "INFO:__main__:Question: Who beat villanova last year in the ncaa tournament?\n",
            "INFO:__main__:Gold answer: The NCAA Division I Men's Basketball Tournament, also known and branded as NCAA March  The Villanova Wildcats have appeared in the NCAA Tournament 39 times, the eighth highest total in NCAA history.  March Madness, is a single-elimination tournament played each spring in the United States, currently featuring 68 college basketball teams from the Division I level of the National Collegiate Athletic Association, NCAA, to determine the national championship. In 2014 The Connecticut Uconn Huskies beat Villanova Wildcats, 2015 they were beat by North Carolina State University Wolfpack and in 2017 the were beat by the Wisconsin Badgers. \n",
            "INFO:__main__:Final model output: The Villanova Wildcats (or 'Cats) men's basketball team, coached by Jay Wright and composed of five starters who played in the NBA. Villanova is best known for their remarkable run to the 2016 national championship, where they defeated the national champion North Carolina Tar Heels, 77–74 at the buzzer after Kris Jenkins took an inbounds pass, stepped behind the three-point line, and sank an improbable shot for the school's second national title and first since 1985. After the game, Coach Wright was named the Associated Press National Coach of the Year. In addition to Kris Jenkins, who was a sophomore at the time, the Wildcats were also led by Phil Booth (senior), Ryan Arcidiacono (senior), Eric Paschall (junior), and Darryl Reynolds II (sophomore).\n",
            " 68%|██████▊   | 34/50 [06:37<02:28,  9.25s/it]INFO:__main__:Prompt length=1134\n",
            "INFO:__main__:Question: Who has scored most goals in international football?\n",
            "INFO:__main__:Gold answer: In total, 67 male footballers to date have managed to score at least 50 goals with their national team at senior level. Ali Daei has scored the most goals in men's international football and is the only player to score over 100 goals in international men's football with 109 goals. Christine Sinclair scored the most goals in international women's football with 187 goals.\n",
            "INFO:__main__:Final model output: \n",
            " 70%|███████   | 35/50 [06:38<01:41,  6.78s/it]INFO:__main__:Prompt length=1138\n",
            "INFO:__main__:Question: When was the 13th amendment ratified by the states?\n",
            "INFO:__main__:Gold answer: The Thirteenth Amendment (Amendment XIII) to the United States Constitution abolished slavery and involuntary servitude, except as punishment for a crime. In Congress, it was passed by the Senate on April 8, 1864, and by the House on January 31, 1865. By the end of February 1865, 18 states had ratified the amendment. At least 27 states had to ratify the amendment for it to come into force, and by December 6, 1865, the amendment was ratified by the required number of states.\n",
            "INFO:__main__:Final model output: The Thirteenth Amendment (Amendment XIII) to the United States Constitution abolished slavery and involuntary servitude, except as punishment for a crime. In Congress, it was passed by the Senate on April 8, 1864, and by the House on January 31, 1865. The amendment was ratified by the required number of states on December 6, 1865, and proclaimed 12 days later. There were about 40,000 slaves in Kentucky and 1,000 in Delaware who were liberated then. As the years went on and American life continued to be deeply unfair towards blacks, cynicism towards Lincoln grew.\n",
            " 72%|███████▏  | 36/50 [06:48<01:47,  7.67s/it]INFO:__main__:Prompt length=1102\n",
            "INFO:__main__:Question: Who is hosting the next world cup 2022?\n",
            "INFO:__main__:Gold answer: The 2022 FIFA World Cup is scheduled to be the 22nd running of the FIFA World Cup competition, the quadrennial international men's football championship contested by the national teams of the member associations of FIFA. It is scheduled to take place in Qatar from 21 November to 18 December 2022. The 2022 Rugby World Cup Sevens will be the eighth edition of the Rugby World Cup Sevens organised by World Rugby. It will take place at the Cape Town Stadium in Capetown, South Africa between 9 and 11 September 2022.\n",
            "INFO:__main__:Final model output:  Russia was chosen to host the 2018 World Cup, and Qatar was chosen to host the 2022 World Cup. This made Russia the first Eastern European country to host the World Cup, while Qatar would be the first Middle Eastern country to host the World Cup. Blatter noted that the committee had decided to \"go to new lands\" and reflected a desire to \"develop football\" by bringing it to more countries. In each round a majority of twelve votes was needed. If no bid received 12 votes in a round, the bid with the fewest votes [2]. If no bid received twelve votes in a round, the bid with the fewest votes [2]\n",
            " 74%|███████▍  | 37/50 [06:58<01:50,  8.49s/it]INFO:__main__:Prompt length=1167\n",
            "INFO:__main__:Question: Who played warden hodges in dad's army?\n",
            "INFO:__main__:Gold answer: Several actors played Warden Hodges in Dad's Army. English comedy actor Bill Pertwee did in the original Dad's Army TV show, a BBC sitcom. He also did in the original Dad's Army radio show and film, a 1971 British war comedy film that was the first film adaptation of the sitcom. English film, stage, and television actor Martin Savage did in the 2016 film Dad's Army, which was also based on the BBC sitcom.\n",
            "INFO:__main__:Final model output: Burberry in London.\n",
            " 76%|███████▌  | 38/50 [06:59<01:16,  6.34s/it]INFO:__main__:Prompt length=1166\n",
            "INFO:__main__:Question: Who's the highest paid nba player 2017?\n",
            "INFO:__main__:Gold answer: The highest paid NBA player in the 2016-2017 season was LeBron James. Recently, the highest-paid NBA players by season has eclipsed $40 million. Stephen Curry became the first player to eclipse $40 million per year when he signed a record 5-year contract worth $201 million in 2017, starting with $34,682,550 in the 2017-18 season.\n",
            "INFO:__main__:Final model output: The highest paid NBA player in 2017 is LeBron James, who made $36,950,000 [3].\n",
            " 78%|███████▊  | 39/50 [07:02<00:57,  5.23s/it]INFO:__main__:Prompt length=1140\n",
            "INFO:__main__:Question: Who conducted a 300 mile march to sacramento california?\n",
            "INFO:__main__:Gold answer: The Delano grape strike was a labor strike organized by the Agricultural Workers Organizing Committee (AWOC), a predominantly Filipino and AFL-CIO-sponsored labor organization, against table grape growers in Delano, California to fight against the exploitation of farm workers. As the strike began to flag in winter, Cesar Chavez decided on a march of 300 miles to the state capitol at Sacramento. This would pass through dozens of farmworker communities and attract attention for their cause.\n",
            "INFO:__main__:Final model output: The Mormon Battalion marched 300 miles to march to their state capital, Selma Alabama [1]. The Mormon Battalion march was one of the longest ever marches of a military unit in United States military history. The Battalion arrived in San Diego on January 29, 1847. The Battalion performed garrison duties in several locations in southern California over the next five months until their discharge on July 16, 1847 in Los Angeles. The Battalion arrived in California on January 29, 1847, after traveling from Iowa and marched for about 1,900 miles until their eventual discharge in Los Angeles [3].\n",
            " 80%|████████  | 40/50 [07:11<01:04,  6.41s/it]INFO:__main__:Prompt length=1161\n",
            "INFO:__main__:Question: Who does the voice of darth vader in star wars?\n",
            "INFO:__main__:Gold answer: Several actors have done the voice of Darth Vader in Star Wars. American actor James Earl Jones voiced Vader in Star Wars Episodes III-VI, IX, Rogue One, and Rebels. American actor and singer Brock Peters voiced Vader in the 1981 radio drama adaptation of Star Wars. Voice actor Ben Burtt voiced Darth Vader's vocal effects in Star Wars.\n",
            "INFO:__main__:Final model output:  Darth Vader has been voiced by James Earl Jones for the 2003 micro-series \" [1](Title: Darth Vader)\" and by Matt Lanter in the CGI animated film \" [2](Title: \"Darth Vader\")\" and for Anakin's cameos in \"Star Wars Rebels.\" For Vader's appearances in the \"Star Wars Rebels\" animated series, James Earl Jones has reprised the voice role. Both Lanter and Jones contributed their voices for the second-season finale of \"Rebels,\" at times with identical dialogue spoken by both actors blended together in different ways. Darth Vader appears in seven of the live-action \"Star Wars\" films, \"\", animated film and \". He also has a recurring role in \"Star Wars\" Expanded Universe.\n",
            " 82%|████████▏ | 41/50 [07:23<01:13,  8.16s/it]INFO:__main__:Prompt length=1136\n",
            "INFO:__main__:Question: Who lived to be the oldest person in the world?\n",
            "INFO:__main__:Gold answer: A supercentenarian is someone who has reached the age of 110. Sarah Knauss, whose age is undisputed, was the oldest person ever from the United States and the second-oldest fully documented person ever. Jeanne Calment was a French supercentenarian and the oldest human whose age is well-documented, with a lifespan of 122 years and 164 days, and was the oldest person in the world as of 1997. In 1985, the oldest living person was Mathew Beard and in 1986 it was Augusta Holtz, who lived 115 years and 79 days, from 1871 to 1986.\n",
            "INFO:__main__:Final model output: The oldest person in the world was Jeanne Calment (1906–1997) [1]. On July 6, 2016, Emma Morano (119) of Italy was declared the world's oldest living person [3].\n",
            " 84%|████████▍ | 42/50 [07:28<00:55,  6.94s/it]INFO:__main__:Prompt length=1155\n",
            "INFO:__main__:Question: When does the 12th day of christmas begin?\n",
            "INFO:__main__:Gold answer: The start and end, or 12th day, of the Twelve Days of Christmas are held to be on a number of different dates. In most Western ecclesiastical traditions, Christmas Day is considered the First Day of Christmas and the Twelve Days are 25 December through 5 January, with January 5 being the final and 12th day of Christmas. In Eastern Christianity, the 12th day is held to be on 18 January. Conversely, the commercial calendar holds that the Twelve Days are 14 December to 25 December, with Christmas Day being the 12th day.\n",
            "INFO:__main__:Final model output: The Twelve Days of Christmas began on Christmas Eve, and lasted till the morning of Epiphany on January 6.\n",
            " 86%|████████▌ | 43/50 [07:30<00:39,  5.63s/it]INFO:__main__:Prompt length=1147\n",
            "INFO:__main__:Question: When did alabama became a right to work state?\n",
            "INFO:__main__:Gold answer: Alabama is a state in the Southeastern region of the United States. In the context of labor law in the United States, the term \"right-to-work laws\" refers to state laws that prohibit union security agreements between employers and labor unions. Alabama became a \"right-to-work\" state in 1953 by adopting statutes. By constitutional provision, Alabama became a \"right-to-work\" state in 2016.\n",
            "INFO:__main__:Final model output: Oklahoma adopted a right-to-work law in 2001 [2]. The right-to-work issue in the legislature was overshadowed thereafter by looming tax increases.\n",
            " 88%|████████▊ | 44/50 [07:34<00:29,  4.98s/it]INFO:__main__:Prompt length=1164\n",
            "INFO:__main__:Question: Who sings i'll be seeing you in the notebook?\n",
            "INFO:__main__:Gold answer: The Notebook is a 2004 American romantic drama film based on the 1996 novel of the same name. The song, \"I'll Be Seeing You\", is heard twice in the movie. The first time the song is heard it is sung by Billie Holiday. The second time the song is heard it is sung by Jimmy Durante.\n",
            "INFO:__main__:Final model output: His Orchestra from 1940 charted in 1944 and peaked at No. 4. I'll Be Seeing You (song) \"I'll Be Seeing You\" is a popular song, with music by Sammy Fain and lyrics by Irving Kahal. Published in 1938, it was inserted into the Broadway musical \"Right This Way\", which closed after fifteen performances. The resemblance between the main tune's first four lines and a passage within the theme of the last movement of Gustav Mahler's Third Symphony (1896) was pointed out by Deryck Cooke in 1970. The recording by Bing Crosby became a hit in 1944, reaching number one for the week of July 1. Frank Sinatra's version with Tommy Dorsey and\n",
            " 90%|█████████ | 45/50 [07:45<00:34,  6.84s/it]INFO:__main__:Prompt length=1119\n",
            "INFO:__main__:Question: Who wrote the book of the new testament?\n",
            "INFO:__main__:Gold answer: The New Testament of the Bible is primarily written in parts with the majority of it written by Jewish disciples of Christ. Matthew, Mark, Luke, John wrote the gospels. The First Epistle of Peter was written by Peter, an apostle of Jesus Christ, Epistle of James was written by James, a servant of God, and the Pauline Epistles was written by Paul of Tarsus.\n",
            "INFO:__main__:Final model output: The New Testament was written by seven men who lived between 55 and 100 years after Jesus' death. They were all Jewish and were part of a church that existed at the time and has existed since. The authors of the four gospels and the remainder of the New Testament books wrote under the authority of other Jewish leaders who had already accepted the teachings of Christ as a way of life.\n",
            " 92%|█████████▏| 46/50 [07:51<00:27,  6.78s/it]INFO:__main__:Prompt length=1159\n",
            "INFO:__main__:Question: The repetition of phrases or sentences with similar structures or meanings is called?\n",
            "INFO:__main__:Gold answer: In rhetoric, Parallel Syntax (also known as parallel construction and parallelism) is a rhetorical device that consists of repetition among adjacent sentences or clauses. The repeated sentences or clauses provides emphasis to a center theme or idea the author is trying to convey. Schesis onomaton is a rhetorical technique used to emphasize an idea by repeating it rapidly using slightly different words that have the same or a very similar meaning.\n",
            "INFO:__main__:Final model output:  Repetition (rhetorical device) [2]. Repetition is the simple repeating of a word, within a short space of words (including in a poem), with no particular placement of the words to secure emphasis. It is a multilinguistic written or spoken device, frequently used in English and several other languages, and so rarely termed a figure of speech. Its forms, many of which are listed below, have varying resonances to listing (forms of enumeration, such as \"Firstly, Secondly, Thirdly and lastly...\"), as a matter of trite logic often similar in effect. It features in famous poems such as: [3](Title: The Life That I Have: The following lines are taken from a poem by Henry Wadsworth Longfellow) The life that I have<br>\n",
            " 94%|█████████▍| 47/50 [08:04<00:25,  8.60s/it]INFO:__main__:Prompt length=1119\n",
            "INFO:__main__:Question: When was the last time the cubs won the world series before 2016?\n",
            "INFO:__main__:Gold answer: The last time the Chicago Cubs won the World Series before 2016 was the 1908 World Series, which was held on October 14, 1908.\n",
            "INFO:__main__:Final model output: The Cubs last won the World Series in 1908. They last won a National League Pennant in 1945. They last won the world series in 2016. The cubs have never won another league pennant since Sianis cursed Billy Goat's.\n",
            " 96%|█████████▌| 48/50 [08:09<00:14,  7.36s/it]INFO:__main__:Prompt length=1136\n",
            "INFO:__main__:Question: Who plays the superintendent in doctor blake mysteries?\n",
            "INFO:__main__:Gold answer: There were several superintendents in the TV series The Doctor Blake Mysteries. Actor Joel Tobeck played Chief Superintendent Matthew Lawson in Series 1-4.1 and 5. New Zealand actor Craig Hall played Chief Superintendent William Munro in Series 3 and 5. Australian actor Rodger Corser played Chief Superintendent Matthew Frank Carlyle in Series 4.\n",
            "INFO:__main__:Final model output: The Doctor Blake Mysteries starring Craig McLachlan has a cast member on the show who is African-American (Jeanne Tripplehorn). [1]\n",
            " 98%|█████████▊| 49/50 [08:12<00:06,  6.12s/it]INFO:__main__:Prompt length=1151\n",
            "INFO:__main__:Question: Who said it is a tale told by an idiot full of sound and fury?\n",
            "INFO:__main__:Gold answer: The play Macbeth has the line \"it is a tale told by an idiot full of sound and fury,\" and Macbeth is the character who speaks it. Lord Macbeth is the title character and main protagonist in William Shakespeare's tragedy Macbeth.\n",
            "INFO:__main__:Final model output:  Macbeth, William Shakespeare (c. 1564–1616) [1]. The title can be extended also to Quentin and Jason, whose respective fates are the focus of the novel. Both men have stories told by \"idiots\" — Quentins is told the truth about the murder of his family at the hands of the \"Rump,\" while Jason is told of his fate by the Compson patriarch [2]. In some senses, the title, the soliloquy, is a lie, but in another sense, the soliloquy is the truth. The novel, itself, is the story of fools (fools who are not fools), and is told by an idiot.\n",
            "100%|██████████| 50/50 [08:23<00:00, 10.08s/it]\n",
            "INFO:__main__:#Cases when prompts exceed max length: 0\n",
            "INFO:__main__:#Cases when max new tokens < 50: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **eval.py**"
      ],
      "metadata": {
        "id": "f8wKxdAF8G5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "id": "Tk-YLakk4gUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Cqo5kODD9G6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import argparse\n",
        "import collections\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "import numpy as np\n",
        "from rouge_score import rouge_scorer, scoring\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "# from utils import normalize_answer, get_max_memory, remove_citations\n",
        "\n",
        "QA_MODEL=\"gaotianyu1350/roberta-large-squad\"\n",
        "AUTOAIS_MODEL=\"google/t5_xxl_true_nli_mixture\"\n",
        "\n",
        "global autoais_model, autoais_tokenizer\n",
        "autoais_model, autoais_tokenizer = None, None\n",
        "\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    \"\"\"Compute F1 score between two strings.\"\"\"\n",
        "\n",
        "    def _get_tokens(s):\n",
        "        if not s:\n",
        "            return []\n",
        "        return normalize_answer(s).split()\n",
        "\n",
        "    gold_toks = _get_tokens(a_gold)\n",
        "    pred_toks = _get_tokens(a_pred)\n",
        "\n",
        "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "        return int(gold_toks == pred_toks)\n",
        "\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1\n",
        "\n",
        "\n",
        "def compute_exact(a_gold, a_pred):\n",
        "    \"\"\"Check whether two strings are equal up to normalization.\"\"\"\n",
        "\n",
        "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "\n",
        "def exact_presence(short_answers, context):\n",
        "    \"\"\"Verify if any of the answers is present in the given context.\n",
        "    Args:\n",
        "        short_answers: list of short answers to look for in the context\n",
        "        context: a paragraph to search for short answers\n",
        "    Returns:\n",
        "        true if any of the short answers is present in the context\n",
        "    \"\"\"\n",
        "\n",
        "    n_short_answers = [normalize_answer(sa) for sa in short_answers]\n",
        "    n_context = normalize_answer(context)\n",
        "\n",
        "    for ans in n_short_answers:\n",
        "        if ans in n_context:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def compute_rouge(data):\n",
        "    \"\"\"Main function for rouge scoring.\n",
        "    If two references are provided,\n",
        "    the best score is chosen for each instance.\n",
        "    Args:\n",
        "        data: requires field `output` and `answer` (or `annotations` for ASQA)\n",
        "        metrics: list of evaluation metrics\n",
        "    Returns:\n",
        "        dictionary representation of rouge scores\n",
        "    \"\"\"\n",
        "    def _rouge_calculation(hypotheses,\n",
        "                        references1,\n",
        "                        references2=[],\n",
        "                        metrics=['rougeLsum']):\n",
        "\n",
        "        if references2 == []:\n",
        "            references2 = references1\n",
        "\n",
        "        scorer = rouge_scorer.RougeScorer(metrics, use_stemmer=True)\n",
        "        aggregator = scoring.BootstrapAggregator()\n",
        "\n",
        "        for i in range(len(hypotheses)):\n",
        "            scores1 = scorer.score(references1[i], hypotheses[i])\n",
        "            scores2 = scorer.score(references2[i], hypotheses[i])\n",
        "            if scores1['rougeLsum'].fmeasure > scores2['rougeLsum'].fmeasure:\n",
        "                aggregator.add_scores(scores1)\n",
        "            else:\n",
        "                aggregator.add_scores(scores2)\n",
        "\n",
        "        scores = {m: [] for m in metrics}\n",
        "\n",
        "        for m in metrics:\n",
        "            fmeasure = aggregator.aggregate()[m].mid.fmeasure\n",
        "            scores[m].append(fmeasure)\n",
        "\n",
        "        for m in scores:\n",
        "            scores[m] = 100 * sum(scores[m]) / len(scores[m])\n",
        "\n",
        "        return scores\n",
        "\n",
        "    hypotheses = {}\n",
        "    references1 = {}\n",
        "    references2 = {}\n",
        "\n",
        "    for idx, item in enumerate(data):\n",
        "        hypotheses[idx] = item[\"output\"]\n",
        "        if \"annotations\" in item and item['annotations'] is not None: # For ASQA\n",
        "            references1[idx] = item[\"annotations\"][0][\"long_answer\"]\n",
        "            references2[idx] = item[\"annotations\"][1][\"long_answer\"]\n",
        "        else:\n",
        "            references1[idx] = item[\"answer\"]\n",
        "            references2[idx] = item[\"answer\"]\n",
        "\n",
        "    h, r1, r2 = [], [], []\n",
        "\n",
        "    for key in references1:\n",
        "        h.append(hypotheses[key])\n",
        "        r1.append(references1[key])\n",
        "\n",
        "        if references2 is not None:\n",
        "            r2.append(references2[key])\n",
        "\n",
        "    h = ['\\n'.join(sent_tokenize(text.lower())) for text in h]\n",
        "    r1 = ['\\n'.join(sent_tokenize(text.lower())) for text in r1]\n",
        "    r2 = ['\\n'.join(sent_tokenize(text.lower())) for text in r2]\n",
        "    scores = _rouge_calculation(h, r1, r2)\n",
        "\n",
        "    return scores['rougeLsum']\n",
        "\n",
        "\n",
        "def compute_str_em(data):\n",
        "    \"\"\"Compute STR-EM metric (only for ASQA)\n",
        "    Args:\n",
        "        data: requires field `qa_pairs/short_answers` and `output`\n",
        "    Returns:\n",
        "        STR-EM and STR-EM-HIT ()\n",
        "    \"\"\"\n",
        "\n",
        "    if 'qa_pairs' not in data[0] or data[0]['qa_pairs'] is None:\n",
        "        return 0, 0\n",
        "\n",
        "    acc = []\n",
        "    hit = []\n",
        "\n",
        "    for item in data:\n",
        "        loc_acc = []\n",
        "        for qa_pair in item['qa_pairs']:\n",
        "            loc_acc.append(exact_presence(qa_pair['short_answers'], item[\"output\"]))\n",
        "        acc.append(np.mean(loc_acc))\n",
        "        hit.append( int(np.mean(loc_acc) == 1) )\n",
        "\n",
        "    return 100 * np.mean(acc), 100 * np.mean(hit)\n",
        "\n",
        "\n",
        "def compute_len(data):\n",
        "    \"\"\"Compute average length of predictions.\"\"\"\n",
        "\n",
        "    res, cntr = 0, 0\n",
        "    for item in data:\n",
        "        res += len(item[\"output\"].split())\n",
        "        cntr += 1\n",
        "    return res / cntr\n",
        "\n",
        "\n",
        "def compute_qa(data):\n",
        "    \"\"\"Compute QA-based accuracy.\n",
        "    Args:\n",
        "        data: requires filed `qa_pairs/short_answers` and `output`\n",
        "    Returns:\n",
        "        QA metrics (QA-EM, QA-F1, QA-Hit)\n",
        "    \"\"\"\n",
        "\n",
        "    if 'qa_pairs' not in data[0] or data[0]['qa_pairs'] is None:\n",
        "        logger.warn(\"Warning: no QA pairs found in data\")\n",
        "        return {\n",
        "            'QA-EM': 0,\n",
        "            'QA-F1': 0,\n",
        "            'QA-Hit': 0,\n",
        "        }\n",
        "\n",
        "    # Load model\n",
        "    logger.info(\"Loading the RoBERTa-large SQuAD model for QA-based accuracy...\")\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=QA_MODEL, device=0)\n",
        "    logger.info(\"Done\")\n",
        "\n",
        "    # Get prediction\n",
        "    logger.info(\"Computing the QA-based accuracy...\")\n",
        "    em, f1, bins = [], [], []\n",
        "    for item in tqdm(data):\n",
        "        question = [qa_pair['question'] for qa_pair in item['qa_pairs']]\n",
        "        context = item['output'] if len(item['output']) > 0 else \" \"\n",
        "        results = qa_pipeline(question=question, context=context, handle_impossible_answer=True)\n",
        "        loc_counter, loc_em, loc_f1 = 0, 0, 0\n",
        "\n",
        "        for idx, res in enumerate(results):\n",
        "            answers = item[\"qa_pairs\"][idx][\"short_answers\"]\n",
        "            prediction = res[\"answer\"]\n",
        "\n",
        "            loc_em += max([compute_exact(a, prediction) for a in answers])\n",
        "            loc_f1 += max([compute_f1(a, prediction) for a in answers])\n",
        "            loc_counter += 1\n",
        "\n",
        "        em.append(loc_em / loc_counter)\n",
        "        f1.append(loc_f1 / loc_counter)\n",
        "        bins.append(loc_em == loc_counter)\n",
        "\n",
        "    return {\n",
        "        'QA-EM': 100 * np.mean(em),\n",
        "        'QA-F1': 100 * np.mean(f1),\n",
        "        'QA-Hit': 100 * np.mean(bins)\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_mauve(data):\n",
        "    \"\"\"Compute Mauve score.\"\"\"\n",
        "\n",
        "    logger.info(\"Computing MAUVE...\")\n",
        "    human_data = []\n",
        "    model_data = []\n",
        "    for item in data:\n",
        "        # Remove ending punctuations\n",
        "        # Remove any new lines\n",
        "        # Truncate by 100 words\n",
        "        human_data.append(' '.join((item['question'] + \" \" + item['answer'].strip()).split()[:100]).rstrip(string.punctuation))\n",
        "        model_data.append(' '.join((item['question'] + \" \" + item['output'].strip()).split()[:100]).rstrip(string.punctuation))\n",
        "\n",
        "    import mauve\n",
        "    out = mauve.compute_mauve(\n",
        "        p_text=human_data,\n",
        "        q_text=model_data,\n",
        "        device_id=0,\n",
        "        max_text_length=512,\n",
        "        verbose=True,\n",
        "        batch_size=8,\n",
        "        featurize_model_name=\"gpt2-large\"\n",
        "    )\n",
        "    return out.mauve * 100\n",
        "\n",
        "\n",
        "def _run_nli_autoais(passage, claim):\n",
        "    \"\"\"\n",
        "    Run inference for assessing AIS between a premise and hypothesis.\n",
        "    Adapted from https://github.com/google-research-datasets/Attributed-QA/blob/main/evaluation.py\n",
        "    \"\"\"\n",
        "    global autoais_model, autoais_tokenizer\n",
        "    input_text = \"premise: {} hypothesis: {}\".format(passage, claim)\n",
        "    input_ids = autoais_tokenizer(input_text, return_tensors=\"pt\").input_ids.to(autoais_model.device)\n",
        "    with torch.inference_mode():\n",
        "        outputs = autoais_model.generate(input_ids, max_new_tokens=10)\n",
        "    result = autoais_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    inference = 1 if result == \"1\" else 0\n",
        "    return inference\n",
        "\n",
        "\n",
        "def compute_claims(data):\n",
        "    global autoais_model, autoais_tokenizer\n",
        "    if autoais_model is None:\n",
        "        logger.info(\"Loading AutoAIS model...\")\n",
        "        autoais_model = AutoModelForSeq2SeqLM.from_pretrained(AUTOAIS_MODEL, torch_dtype=torch.bfloat16, max_memory=get_max_memory(), device_map=\"auto\")\n",
        "        autoais_tokenizer = AutoTokenizer.from_pretrained(AUTOAIS_MODEL, use_fast=False)\n",
        "\n",
        "    logger.info(\"Computing claims...\")\n",
        "    scores = []\n",
        "    for item in tqdm(data):\n",
        "        normalized_output = remove_citations(item['output'])\n",
        "        entail = 0\n",
        "        claims = item[\"claims\"]\n",
        "        for claim in claims:\n",
        "            entail += _run_nli_autoais(normalized_output, claim)\n",
        "        scores.append(entail / len(claims))\n",
        "    return 100 * np.mean(scores)\n",
        "\n",
        "\n",
        "def compute_autoais(data,\n",
        "                    decontext=False,\n",
        "                    concat=False,\n",
        "                    qampari=False,\n",
        "                    at_most_citations=None,):\n",
        "    \"\"\"\n",
        "    Compute AutoAIS score.\n",
        "\n",
        "    Args:\n",
        "        data: requires field `output` and `docs`\n",
        "              - docs should be a list of items with fields `title` and `text` (or `phrase` and `sent` for QA-extracted docs)\n",
        "        citation: check citations and use the corresponding references.\n",
        "        decontext: decontextualize the output\n",
        "    \"\"\"\n",
        "\n",
        "    global autoais_model, autoais_tokenizer\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # Added !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    if autoais_model is None:\n",
        "        logger.info(\"Loading AutoAIS model...\")\n",
        "        autoais_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            AUTOAIS_MODEL,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            max_memory=get_max_memory(),\n",
        "            offload_folder='offload_folder2', # Added !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "            device_map=\"auto\",\n",
        "#             device_map=device,\n",
        "        )\n",
        "        autoais_tokenizer = AutoTokenizer.from_pretrained(AUTOAIS_MODEL, use_fast=False)\n",
        "\n",
        "    logger.info(f\"Running AutoAIS...\")\n",
        "\n",
        "    def _format_document(doc):\n",
        "        \"\"\"Format document for AutoAIS.\"\"\"\n",
        "\n",
        "        if \"sent\" in doc:\n",
        "            # QA-extracted docs\n",
        "            return \"Title: %s\\n%s\" % (doc['title'], doc['sent'])\n",
        "        else:\n",
        "            return \"Title: %s\\n%s\" % (doc['title'], doc['text'])\n",
        "\n",
        "    ais_scores = []\n",
        "    ais_scores_prec = []\n",
        "\n",
        "    sent_total = 0\n",
        "    sent_mcite = 0\n",
        "    sent_mcite_support = 0\n",
        "    sent_mcite_overcite = 0\n",
        "    autoais_log = []\n",
        "    for item in tqdm(data):\n",
        "        # Get sentences by using NLTK\n",
        "        if qampari:\n",
        "            sents = [item['question'] + \" \" + x.strip() for x in item['output'].rstrip().rstrip(\".\").rstrip(\",\").split(\",\")]\n",
        "        else:\n",
        "            sents = sent_tokenize(item['output'])\n",
        "        if len(sents) == 0:\n",
        "            continue\n",
        "\n",
        "        target_sents = [remove_citations(sent).strip() for sent in sents]\n",
        "\n",
        "        entail = 0\n",
        "        entail_prec = 0\n",
        "        total_citations = 0\n",
        "        for sent_id, sent in enumerate(sents):\n",
        "            target_sent = target_sents[sent_id] # Citation removed and (if opted for) decontextualized\n",
        "            joint_entail = -1 # Undecided\n",
        "\n",
        "            # Find references\n",
        "            ref = [int(r[1:])-1 for r in re.findall(r\"\\[\\d+\", sent)] # In text citation id starts from 1\n",
        "            logger.info(f\"For `{sent}`, find citations {ref}\")\n",
        "            if len(ref) == 0:\n",
        "                # No citations\n",
        "                joint_entail = 0\n",
        "            elif any([ref_id >= len(item['docs']) for ref_id in ref]):\n",
        "                # Citations out of range\n",
        "                joint_entail = 0\n",
        "            else:\n",
        "                if at_most_citations is not None:\n",
        "                    ref = ref[:at_most_citations]\n",
        "                total_citations += len(ref)\n",
        "                joint_passage = '\\n'.join([_format_document(item['docs'][psgs_id]) for psgs_id in ref])\n",
        "\n",
        "            # If not directly rejected by citation format error, calculate the recall score\n",
        "            if joint_entail == -1:\n",
        "                joint_entail = _run_nli_autoais(joint_passage, target_sent)\n",
        "                autoais_log.append({\n",
        "                    \"question\": item['question'],\n",
        "                    \"output\": item['output'],\n",
        "                    \"claim\": sent,\n",
        "                    \"passage\": [joint_passage],\n",
        "                    \"model_type\": \"NLI\",\n",
        "                    \"model_output\": joint_entail,\n",
        "                })\n",
        "\n",
        "            entail += joint_entail\n",
        "            if len(ref) > 1:\n",
        "                sent_mcite += 1\n",
        "\n",
        "            # calculate the precision score if applicable\n",
        "            if joint_entail and len(ref) > 1:\n",
        "                sent_mcite_support += 1\n",
        "                # Precision check: did the model cite any unnecessary documents?\n",
        "                for psgs_id in ref:\n",
        "                    # condition A\n",
        "                    passage = _format_document(item['docs'][psgs_id])\n",
        "                    nli_result = _run_nli_autoais(passage, target_sent)\n",
        "\n",
        "                    # condition B\n",
        "                    if not nli_result:\n",
        "                        subset_exclude = copy.deepcopy(ref)\n",
        "                        subset_exclude.remove(psgs_id)\n",
        "                        passage = '\\n'.join([_format_document(item['docs'][pid]) for pid in subset_exclude])\n",
        "                        nli_result = _run_nli_autoais(passage, target_sent)\n",
        "                        if nli_result: # psgs_id is not necessary\n",
        "                            flag = 0\n",
        "                            sent_mcite_overcite += 1\n",
        "                        else:\n",
        "                            entail_prec += 1\n",
        "                    else:\n",
        "                        entail_prec += 1\n",
        "            else:\n",
        "                entail_prec += joint_entail\n",
        "\n",
        "        sent_total += len(sents)\n",
        "        ais_scores.append(entail / len(sents))\n",
        "        ais_scores_prec.append(entail_prec / total_citations if total_citations > 0 else 0) # len(sents))\n",
        "\n",
        "    if sent_mcite > 0 and sent_mcite_support > 0:\n",
        "        print(\"Among all sentences, %.2f%% have multiple citations, among which %.2f%% are supported by the joint set, among which %.2f%% overcite.\" % (\n",
        "            100 * sent_mcite / sent_total,\n",
        "            100 * sent_mcite_support / sent_mcite,\n",
        "            100 * sent_mcite_overcite / sent_mcite_support\n",
        "        ))\n",
        "\n",
        "    return {\n",
        "        \"citation_rec\": 100 * np.mean(ais_scores),\n",
        "        \"citation_prec\": 100 * np.mean(ais_scores_prec),\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_qampari_f1(data, cot=False):\n",
        "    prec = []\n",
        "    rec = []\n",
        "    rec_top5 = []\n",
        "    f1 = []\n",
        "    f1_top5 = []\n",
        "\n",
        "    num_preds = []\n",
        "    for item in data:\n",
        "        if cot:\n",
        "            if \":\" in item['output']:\n",
        "                o = ':'.join(item['output'].split(\":\")[1:]) # try to separate the COT part and the answer list part.\n",
        "            else:\n",
        "                o = \"\"\n",
        "        else:\n",
        "            o = item['output']\n",
        "        preds = [normalize_answer(x.strip()) for x in o.rstrip().rstrip(\".\").rstrip(\",\").split(\",\")]\n",
        "        preds = [p for p in preds if len(p) > 0] # delete empty answers\n",
        "        num_preds.append(len(preds))\n",
        "        answers = [[normalize_answer(x) for x in ans] for ans in item['answers']]\n",
        "        flat_answers = [item for sublist in answers for item in sublist]\n",
        "\n",
        "        prec.append(sum([p in flat_answers for p in preds]) / len(preds) if len(preds) > 0 else 0)\n",
        "        rec.append(sum([any([x in preds for x in a]) for a in answers]) / len(answers))\n",
        "        rec_top5.append(min(5, sum([any([x in preds for x in a]) for a in answers])) / min(5, len(answers)))\n",
        "        if (prec[-1] + rec[-1]) == 0:\n",
        "            f1.append(0)\n",
        "        else:\n",
        "            f1.append(2 * prec[-1] * rec[-1] / (prec[-1] + rec[-1]))\n",
        "        if (prec[-1] + rec_top5[-1]) == 0:\n",
        "            f1_top5.append(0)\n",
        "        else:\n",
        "            f1_top5.append(2 * prec[-1] * rec_top5[-1] / (prec[-1] + rec_top5[-1]))\n",
        "\n",
        "    return {\n",
        "        \"num_preds\": np.mean(num_preds),\n",
        "        \"qampari_prec\": 100 * np.mean(prec),\n",
        "        \"qampari_rec\": 100 * np.mean(rec),\n",
        "        \"qampari_rec_top5\": 100 * np.mean(rec_top5),\n",
        "        \"qampari_f1\": 100 * np.mean(f1),\n",
        "        \"qampari_f1_top5\": 100 * np.mean(f1_top5),\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--f\", type=str, required=True, help=\"Output file. Should have field `question`, `output`, (ROUGE) `answer`, \\\n",
        "                        (accuracy) `qa_pairs`, (AIS) `docs`\")\n",
        "    parser.add_argument(\"--no_rouge\", action=\"store_true\", help=\"Do not evaluate ROUGE score\")\n",
        "    parser.add_argument(\"--qa\", action=\"store_true\", help=\"Use the QA model\")\n",
        "    parser.add_argument(\"--mauve\", action=\"store_true\", help=\"Use the mauve score model\")\n",
        "    parser.add_argument(\"--citations\", action=\"store_true\", help=\"Evaluation with citation\")\n",
        "    parser.add_argument(\"--at_most_citations\", type=int, default=3, help=\"At most take this many documents (mostly for precision)\")\n",
        "    parser.add_argument(\"--claims_nli\", action=\"store_true\", help=\"Use claims for ELI5\")\n",
        "\n",
        "    # QAMPARI\n",
        "    parser.add_argument(\"--cot\", action=\"store_true\", help=\"For QAMPARI, try to find colon and separate the COT and answer listing\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    with open(args.f) as f:\n",
        "        data_with_config = json.load(f)\n",
        "    data = data_with_config['data']\n",
        "\n",
        "    if \"qampari\" in args.f:\n",
        "        args.no_rouge = True\n",
        "        args.qa = False\n",
        "        args.mauve = False\n",
        "        args.decontext = False\n",
        "        qampari = True\n",
        "    else:\n",
        "        qampari = False\n",
        "\n",
        "    # Truncate by newline and remove on the fly search result\n",
        "    logger.warning(\"We remove all the pre/appended space/newlines and we truncate the answer by the first newline.\")\n",
        "    logger.warning(\"We replace any on the fly search result to standard bracket citation format.\")\n",
        "    for i in range(len(data)):\n",
        "        data[i]['output'] = data[i]['output'].strip().split(\"\\n\")[0]\n",
        "        data[i]['output'] = data[i]['output'].replace(\"<|im_end|>\", \"\")\n",
        "\n",
        "\n",
        "    # Remove all citations for all non-AutoAIS evaluation\n",
        "    normalized_data = copy.deepcopy(data)\n",
        "    for i in range(len(normalized_data)):\n",
        "        normalized_data[i]['output'] = remove_citations(normalized_data[i]['output'])\n",
        "\n",
        "    result = {}\n",
        "    result['length'] = compute_len(normalized_data)\n",
        "    result['str_em'], result['str_hit'] = compute_str_em(normalized_data)\n",
        "    if qampari:\n",
        "        result.update(compute_qampari_f1(normalized_data, cot=args.cot))\n",
        "    if not args.no_rouge:\n",
        "        result['rougeLsum'] = compute_rouge(normalized_data)\n",
        "    if args.qa:\n",
        "        result.update(compute_qa(normalized_data))\n",
        "    if args.mauve:\n",
        "        result['mauve'] = compute_mauve(normalized_data)\n",
        "    if args.citations:\n",
        "        result.update(compute_autoais(data, qampari=qampari, at_most_citations=args.at_most_citations))\n",
        "    if args.claims_nli:\n",
        "        result[\"claims_nli\"] = compute_claims(normalized_data)\n",
        "\n",
        "    print(result)\n",
        "    with open(args.f + \".score\", \"w\") as f:\n",
        "        json.dump(result, f, indent=4)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sys.argv = \"eval.py --f ./result/asqa-opt-6.7b-gtr-shot1-ndoc3-42.json --citations\".split()\n",
        "    main()"
      ],
      "metadata": {
        "id": "a_gxs7Yd8NzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UDyXHSR_9KKl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}