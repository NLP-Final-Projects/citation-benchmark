{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qXYB7HIT1-sB","trusted":true},"outputs":[],"source":["!pip install spacy\n","!python -m spacy download en_core_web_sm\n","!pip install bitsandbytes"]},{"cell_type":"markdown","metadata":{"id":"C4CYHeXJvygY"},"source":["# **My Code**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z46oP12xvsqk","trusted":true},"outputs":[],"source":["!mkdir prompts\n","!gdown 1o5nAmvgI0RnNhvac2gALFDlRgNcfZvsu\n","!mv prompts.zip ./prompts/\n","%cd prompts\n","!unzip prompts.zip\n","!rm prompts.zip\n","%cd .."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"joI8CC8jPwJj","trusted":true},"outputs":[],"source":["!mkdir configs\n","!gdown 1yQQDa0BhNb930_oBrYIJ41CksRENQvbU\n","!mv configs.zip ./configs/\n","%cd configs\n","!unzip configs.zip\n","!rm configs.zip\n","%cd .."]},{"cell_type":"markdown","metadata":{"id":"MwY33ndowL8K"},"source":["# **download_data.sh**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cDCOjaiwD5m","trusted":true},"outputs":[],"source":["!wget https://huggingface.co/datasets/princeton-nlp/ALCE-data/resolve/main/ALCE-data.tar\n","!tar xvf ALCE-data.tar\n","!mv ALCE-data data\n","!rm ALCE-data.tar"]},{"cell_type":"markdown","metadata":{"id":"0lqYT7CKwY8w"},"source":["# **utils.py**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-15T15:01:40.358684Z","iopub.status.busy":"2024-08-15T15:01:40.358175Z","iopub.status.idle":"2024-08-15T15:01:42.242974Z","shell.execute_reply":"2024-08-15T15:01:42.242109Z","shell.execute_reply.started":"2024-08-15T15:01:40.358639Z"},"id":"tA1SI-q6wX1p","trusted":true},"outputs":[],"source":["import logging\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","\n","import torch\n","import json\n","import re\n","import os\n","import string\n","import time\n","\n","def normalize_answer(s):\n","    def remove_articles(text):\n","        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n","\n","    def white_space_fix(text):\n","        return \" \".join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return \"\".join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","\n","def remove_citations(sent):\n","    return re.sub(r\"\\[\\d+\", \"\", re.sub(r\" \\[\\d+\", \"\", sent)).replace(\" |\", \"\").replace(\"]\", \"\")\n","\n","\n","def get_max_memory():\n","    \"\"\"Get the maximum memory available for the current GPU for loading models.\"\"\"\n","    free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n","    max_memory = f'{free_in_GB-6}GB'\n","    n_gpus = torch.cuda.device_count()\n","    max_memory = {i: max_memory for i in range(n_gpus)}\n","    return max_memory\n","\n","\n","def make_doc_prompt(doc, doc_id, doc_prompt, use_shorter=None):\n","    # For doc prompt:\n","    # - {ID}: doc id (starting from 1)\n","    # - {T}: title\n","    # - {P}: text\n","    # use_shorter: None, \"summary\", or \"extraction\"\n","\n","    text = doc['text']\n","    if use_shorter is not None:\n","        text = doc[use_shorter]\n","    return doc_prompt.replace(\"{T}\", doc[\"title\"]).replace(\"{P}\", text).replace(\"{ID}\", str(doc_id+1))\n","\n","\n","def get_shorter_text(item, docs, ndoc, key):\n","    doc_list = []\n","    for item_id, item in enumerate(docs):\n","        if key not in item:\n","            if len(doc_list) == 0:\n","                # If there aren't any document, at least provide one (using full text)\n","                item[key] = item['text']\n","                doc_list.append(item)\n","            logger.warn(f\"No {key} found in document. It could be this data do not contain {key} or previous documents are not relevant. This is document {item_id}. This question will only have {len(doc_list)} documents.\")\n","            break\n","        if \"irrelevant\" in item[key] or \"Irrelevant\" in item[key]:\n","            continue\n","        doc_list.append(item)\n","        if len(doc_list) >= ndoc:\n","            break\n","    return doc_list\n","\n","\n","def make_demo(item, prompt, ndoc=None, doc_prompt=None, instruction=None, use_shorter=None, test=False):\n","    # For demo prompt\n","    # - {INST}: the instruction\n","    # - {D}: the documents\n","    # - {Q}: the question\n","    # - {A}: the answers\n","    # ndoc: number of documents to put in context\n","    # use_shorter: None, \"summary\", or \"extraction\"\n","\n","    prompt = prompt.replace(\"{INST}\", instruction).replace(\"{Q}\", item['question'])\n","    if \"{D}\" in prompt:\n","        if ndoc == 0:\n","            prompt = prompt.replace(\"{D}\\n\", \"\") # if there is no doc we also delete the empty line\n","        else:\n","            doc_list = get_shorter_text(item, item[\"docs\"], ndoc, use_shorter) if use_shorter is not None else item[\"docs\"][:ndoc]\n","            text = \"\".join([make_doc_prompt(doc, doc_id, doc_prompt, use_shorter=use_shorter) for doc_id, doc in enumerate(doc_list)])\n","            prompt = prompt.replace(\"{D}\", text)\n","\n","    if not test:\n","        answer = \"\\n\" + \"\\n\".join(item[\"answer\"]) if isinstance(item[\"answer\"], list) else item[\"answer\"]\n","        prompt = prompt.replace(\"{A}\", \"\").rstrip() + answer\n","    else:\n","        prompt = prompt.replace(\"{A}\", \"\").rstrip() # remove any space or \\n\n","\n","    return prompt\n","\n","\n","def load_model(model_name_or_path, dtype=torch.float16, int8=False, reserve_memory=10):\n","    # Load a huggingface model and tokenizer\n","    # dtype: torch.float16 or torch.bfloat16\n","    # int8: whether to use int8 quantization\n","    # reserve_memory: how much memory to reserve for the model on each gpu (in GB)\n","\n","    # Load the FP16 model\n","    from transformers import AutoModelForCausalLM, AutoTokenizer\n","    logger.info(f\"Loading {model_name_or_path} in {dtype}...\")\n","    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # Added !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","    if int8:\n","        logger.warn(\"Use LLM.int8\")\n","    start_time = time.time()\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_name_or_path,\n","#         device_map='auto',\n","        device_map=device,\n","        torch_dtype=dtype,\n","        max_memory=get_max_memory(),\n","        load_in_8bit=int8,\n","        offload_folder='offload_folder', # Added !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","    )\n","    logger.info(\"Finish loading in %.2f sec.\" % (time.time() - start_time))\n","\n","    # Load the tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)\n","\n","    # Fix OPT bos token problem in HF\n","    if \"opt\" in model_name_or_path:\n","        tokenizer.bos_token = \"<s>\"\n","    tokenizer.padding_side = \"left\"\n","\n","    return model, tokenizer"]},{"cell_type":"markdown","metadata":{"id":"OIb6oljrwsQZ"},"source":["# **searcher.py**"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-15T15:01:47.233156Z","iopub.status.busy":"2024-08-15T15:01:47.232366Z","iopub.status.idle":"2024-08-15T15:01:47.834640Z","shell.execute_reply":"2024-08-15T15:01:47.833822Z","shell.execute_reply.started":"2024-08-15T15:01:47.233122Z"},"id":"TeRJ3B0-wp0r","trusted":true},"outputs":[],"source":["import json\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import torch\n","from torch.nn.functional import normalize\n","\n","def doc_to_text_tfidf(doc):\n","    return doc['title'] + ' ' + doc['text']\n","\n","def doc_to_text_dense(doc):\n","    return doc['title'] + '. ' + doc['text']\n","\n","\n","class SearcherWithinDocs:\n","\n","    def __init__(self, docs, retriever, model=None, device=\"cuda\"):\n","        self.retriever = retriever\n","        self.docs = docs\n","        self.device = device\n","        if retriever == \"tfidf\":\n","            self.tfidf = TfidfVectorizer()\n","            self.tfidf_docs = self.tfidf.fit_transform([doc_to_text_tfidf(doc) for doc in docs])\n","        elif \"gtr\" in retriever:\n","            self.model = model\n","            self.embeddings = self.model.encode([doc_to_text_dense(doc) for doc in docs], device=self.device, convert_to_numpy=False, convert_to_tensor=True, normalize_embeddings=True)\n","        else:\n","            raise NotImplementedError\n","\n","    def search(self, query):\n","        # Return the top-1 result doc id\n","\n","        if self.retriever == \"tfidf\":\n","            tfidf_query = self.tfidf.transform([query])[0]\n","            similarities = [cosine_similarity(tfidf_doc, tfidf_query) for tfidf_doc in self.tfidf_docs]\n","            best_doc_id = np.argmax(similarities)\n","            return best_doc_id\n","        elif \"gtr\" in self.retriever:\n","            q_embed = self.model.encode([query], device=self.device, convert_to_numpy=False, convert_to_tensor=True, normalize_embeddings=True)\n","            score = torch.matmul(self.embeddings, q_embed.t()).squeeze(1).detach().cpu().numpy()\n","            best_doc_id = np.argmax(score)\n","            return best_doc_id\n","        else:\n","            raise NotImplementedError"]},{"cell_type":"markdown","metadata":{"id":"UsDWOD6havmg"},"source":["# **Split Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oD4bat313SvD","trusted":true},"outputs":[],"source":["%mv ./data/asqa_eval_gtr_top100.json ./data/asqa_eval_gtr_top100_original.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHdS-PCn3wrN","trusted":true},"outputs":[],"source":["import json\n","\n","f = open('./data/asqa_eval_gtr_top100_original.json')\n","data = json.load(f)\n","f.close()\n","\n","new_data = data[:50]\n","f = open('./data/asqa_eval_gtr_top100.json', \"w\")\n","json.dump(new_data, f)\n","f.close()"]},{"cell_type":"markdown","metadata":{"id":"OGx6Hw6qbAI_"},"source":["# **Change Summary**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YYJrvhIrLUrm","trusted":true},"outputs":[],"source":["f = open('./data/asqa_eval_gtr_top100_original.json')\n","data = json.load(f)\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZyrvdzELWR0","trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","\n","lengths = list()\n","for i, q in enumerate(data):\n","    docs = q['docs']\n","    for j, doc in enumerate(docs):\n","        text = doc['text']\n","        lengths.append(len(text))\n","\n","threshold = np.percentile(lengths, 80)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QTkARm6gbDNO","trusted":true},"outputs":[],"source":["f = open('./data/asqa_eval_gtr_top100.json')\n","data = json.load(f)\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUJPpqYz2LZZ","trusted":true},"outputs":[],"source":["import spacy\n","import random\n","\n","def reconstruct_sentence(sentence, percentage=0.25):\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    doc = nlp(sentence)\n","\n","    new_sentence_words = []\n","    for token in doc:\n","        if token.ent_iob_ == 'B' or token.ent_iob_ == 'I':\n","            if not new_sentence_words or new_sentence_words[-1] != token.ent_iob_:\n","                new_sentence_words.append(token.text)\n","        else:\n","            if random.random() < percentage:\n","                new_sentence_words.append(token.text)\n","\n","    return new_sentence_words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oHzeO8Uu2p1a","trusted":true},"outputs":[],"source":["import os\n","\n","hf_access_token = \"YOUR_TOKEN\"\n","os.environ[\"HF_ACCESS_TOKEN\"] = hf_access_token"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import spacy\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from transformers import BitsAndBytesConfig"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["nlp = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1_ieeJW2r_A","trusted":true},"outputs":[],"source":["config = BitsAndBytesConfig(\n","    load_in_8bit=True,\n",")\n","model_name = \"meta-llama/Meta-Llama-3-8B\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    device_map=\"auto\",\n","    quantization_config=config,\n","    use_cache=False,\n","    attn_implementation=\"sdpa\",\n","    torch_dtype=torch.float16\n",")\n","model.generation_config.pad_token_id = tokenizer.pad_token_id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-p_k1gF93ck6","trusted":true},"outputs":[],"source":["def generate_sentence_with_entities(entities):\n","    prompt = f\"Generate a sentence using the following entities: {', '.join(entities)}.\"\n","\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","    outputs = model.generate(\n","        **inputs,\n","        max_length=500,\n","        num_return_sequences=1,\n","        no_repeat_ngram_size=2,\n","        num_beams=5,\n","        early_stopping=True\n","    )\n","\n","    generated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return generated_sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vx6bgD2bNVI","trusted":true},"outputs":[],"source":["for i, q in enumerate(data):\n","    docs = q['docs']\n","    for j, doc in tqdm(enumerate(docs)):\n","        text = doc['text']\n","        if len(text) > threshold:\n","            new_text = reconstruct_sentence(text)\n","            generated_sentence = generate_sentence_with_entities(new_text)\n","            data[i]['docs'][j]['text'] = generated_sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGvSl9Vd11WJ","trusted":true},"outputs":[],"source":["f = open('./data/asqa_eval_gtr_top100.json', \"w\")\n","json.dump(data, f)\n","f.close()"]},{"cell_type":"markdown","metadata":{"id":"a5o9EwScwyHA"},"source":["# **run.py**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6nmh8OF0w0Qw","trusted":true},"outputs":[],"source":["!pip install openai safetensors"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["eedc5afd201345229a9ee82ff8bc30ce","1263bbd624f44d168e9e70d1371e8b26","e3c70a1c828441eeb26763c7dd26ef5f","cb0cf9cf297945adb1fbe1e4b92d6987","6eb11b65f29b4a5e9cb175f9916e5b12","cd3b87406fcc496b82ecefab683efeb6","b4876cd4a8af46119941b763bd5738d0","7d3a59579ed942f7a1c47998a1eb7db3","241504e2b1da4c9297b4ee6ed37f9fe8","1ac81e0f1bef40dab07f6c7e6086e590","d521c60c368546578607378ed6073b37"]},"id":"2qH4ZOarxFJ4","outputId":"11cf79f9-049d-4d4f-df59-a18d6ee3afa5","trusted":true},"outputs":[],"source":["import sys\n","import logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","\n","import argparse\n","import os\n","import openai\n","import json\n","from tqdm import tqdm\n","from transformers import AutoTokenizer\n","import time\n","import string\n","import numpy as np\n","import re\n","# from searcher import SearcherWithinDocs\n","import yaml\n","# from utils import *\n","from nltk import sent_tokenize\n","\n","def remove_citations(sent):\n","    return re.sub(r\"\\[\\d+\", \"\", re.sub(r\" \\[\\d+\", \"\", sent)).replace(\" |\", \"\").replace(\"]\", \"\")\n","\n","class LLM:\n","\n","    def __init__(self, args):\n","        self.args = args\n","\n","        if args.openai_api:\n","            import openai\n","            OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n","            OPENAI_ORG_ID = os.environ.get(\"OPENAI_ORG_ID\")\n","            OPENAI_API_BASE = os.environ.get(\"OPENAI_API_BASE\")\n","\n","            if args.azure:\n","                openai.api_key = OPENAI_API_KEY\n","                openai.api_base = OPENAI_API_BASE\n","                openai.api_type = 'azure'\n","                openai.api_version = '2023-05-15'\n","            else:\n","                openai.api_key = OPENAI_API_KEY\n","                openai.organization = OPENAI_ORG_ID\n","\n","            self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", fast_tokenizer=False) # TODO: For ChatGPT we should use a different one\n","            # To keep track of how much the API costs\n","            self.prompt_tokens = 0\n","            self.completion_tokens = 0\n","        else:\n","            self.model, self.tokenizer = load_model(args.model)\n","\n","        self.prompt_exceed_max_length = 0\n","        self.fewer_than_50 = 0\n","        self.azure_filter_fail = 0\n","\n","\n","    def generate(self, prompt, max_tokens, stop=None):\n","        args = self.args\n","        if max_tokens <= 0:\n","            self.prompt_exceed_max_length += 1\n","            logger.warning(\"Prompt exceeds max length and return an empty string as answer. If this happens too many times, it is suggested to make the prompt shorter\")\n","            return \"\"\n","        if max_tokens < 50:\n","            self.fewer_than_50 += 1\n","            logger.warning(\"The model can at most generate < 50 tokens. If this happens too many times, it is suggested to make the prompt shorter\")\n","\n","        if args.openai_api:\n","            use_chat_api = (\"turbo\" in args.model and not args.azure) or (\"gpt-4\" in args.model and args.azure)\n","            if use_chat_api:\n","                # For chat API, we need to convert text prompts to chat prompts\n","                prompt = [\n","                    {'role': 'system', 'content': \"You are a helpful assistant that answers the following questions with proper citations.\"},\n","                    {'role': 'user', 'content': prompt}\n","                ]\n","            if args.azure:\n","                deploy_name = args.model\n","\n","            if use_chat_api:\n","                is_ok = False\n","                retry_count = 0\n","                while not is_ok:\n","                    retry_count += 1\n","                    try:\n","                        response = openai.ChatCompletion.create(\n","                            engine=deploy_name if args.azure else None,\n","                            model=args.model,\n","                            messages=prompt,\n","                            temperature=args.temperature,\n","                            max_tokens=max_tokens,\n","                            stop=stop,\n","                            top_p=args.top_p,\n","                        )\n","                        is_ok = True\n","                    except Exception as error:\n","                        if retry_count <= 5:\n","                            logger.warning(f\"OpenAI API retry for {retry_count} times ({error})\")\n","                            continue\n","                        print(error)\n","                        import pdb; pdb.set_trace()\n","                self.prompt_tokens += response['usage']['prompt_tokens']\n","                self.completion_tokens += response['usage']['completion_tokens']\n","                return response['choices'][0]['message']['content']\n","            else:\n","                is_ok = False\n","                retry_count = 0\n","                while not is_ok:\n","                    retry_count += 1\n","                    try:\n","                        response = openai.Completion.create(\n","                            engine=deploy_name if args.azure else None,\n","                            model=args.model,\n","                            prompt=prompt,\n","                            temperature=args.temperature,\n","                            max_tokens=max_tokens,\n","                            top_p=args.top_p,\n","                            stop=[\"\\n\", \"\\n\\n\"] + (stop if stop is not None else [])\n","                        )\n","                        is_ok = True\n","                    except Exception as error:\n","                        if retry_count <= 5:\n","                            logger.warning(f\"OpenAI API retry for {retry_count} times ({error})\")\n","                            if \"triggering Azure OpenAI’s content management policy\" in str(error):\n","                                # filtered by Azure\n","                                self.azure_filter_fail += 1\n","                                return \"\"\n","                            continue\n","                        print(error)\n","                        import pdb; pdb.set_trace()\n","                self.prompt_tokens += response['usage']['prompt_tokens']\n","                self.completion_tokens += response['usage']['completion_tokens']\n","                return response['choices'][0]['text']\n","        else:\n","            inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(self.model.device)\n","            stop = [] if stop is None else stop\n","            stop = list(set(stop + [\"\\n\", \"Ċ\", \"ĊĊ\", \"<0x0A>\"])) # In Llama \\n is <0x0A>; In OPT \\n is Ċ\n","            stop_token_ids = list(set([self.tokenizer._convert_token_to_id(stop_token) for stop_token in stop] + [self.model.config.eos_token_id]))\n","            if \"llama\" in args.model.lower():\n","                stop_token_ids.remove(self.tokenizer.unk_token_id)\n","            outputs = self.model.generate(\n","                **inputs,\n","                do_sample=True, temperature=args.temperature, top_p=args.top_p,\n","                max_new_tokens=max_tokens,\n","                num_return_sequences=1,\n","                eos_token_id=stop_token_ids\n","            )\n","            generation = self.tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n","            return generation\n","\n","\n","def main():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--config\", type=str, default=None, help=\"Path to the config file\")\n","\n","    # Prompt file is a json file that contains the following fields:\n","    # - instruction: the instruction, which will appear at the beginning of each demo and the test example\n","    # - demo_sep: the separator between each demo, for example, \"\\n\\n\\n\"\n","    # - demo_prompt: the prompt for the demo, for example, \"Instruction: {INST}\\n\\nQuestion: {Q}\\n\\n{D}\\nAnswer: {A}\"\n","    #     - {INST}: the instruction\n","    #     - {D}: the documents\n","    #     - {Q}: the question\n","    #     - {A}: the answers\n","    # - doc_prompt, the prompt for each document, for example, \"Document [{ID}](Title: {T}): {P}\", where\n","    #     - {ID}: the document id, staring from 1\n","    #     - {T}: the document title\n","    #     - {P}: the document text\n","    # - demos: a list of demo examples, each of which should have\n","    #     - question: the question\n","    #     - docs: the documents (\"title\" and \"text\")\n","    #     - answer: the answer to show in the demo. If it is a list, they will be concatenated by \"\\n\". This is useful when the answer includes interactive components.\n","    # Note that this python file will sample `--shot` demos from the prompt file given the random seed `--seed`\n","    parser.add_argument(\"--prompt_file\", type=str, help=\"Path to the prompt file\")\n","\n","    # Evaluation file is a json file that contains a list of item, each of which contains\n","    # - question: the question\n","    # - answer: the answer\n","    # - docs: the documents, each of which contains \"title\", \"text\"\n","    parser.add_argument(\"--eval_file\", type=str, help=\"Path to the eval file\")\n","    parser.add_argument(\"--quick_test\", type=int, default=None, help=\"Quickly test a few examples\")\n","\n","    # ICL setting\n","    parser.add_argument(\"--ndoc\", type=int, help=\"Number of documents\")\n","    parser.add_argument(\"--shot\", type=int, help=\"Number of ICL demonstrations\")\n","    parser.add_argument(\"--seed\", type=int, default=42, help=\"Seed for the random number generator\")\n","    parser.add_argument(\"--no_doc_in_demo\", type=bool, default=False, help=\"Whether to remove the documents in the demos\")\n","    parser.add_argument(\"--fewer_doc_in_demo\", type=bool, default=False, help=\"Whether to use fewer documents in the demos\")\n","    parser.add_argument(\"--ndoc_in_demo\", type=int, default=None, help=\"When using --fewer_doc_in_demo, use this to designate how many docs in demo\")\n","\n","    # Model and name\n","    parser.add_argument(\"--dataset_name\", type=str, help=\"Name of the dataset (for saving)\")\n","    parser.add_argument(\"--tag\", type=str, help=\"Tag of run (for saving)\")\n","    parser.add_argument(\"--model\", type=str, help=\"Model to use\")\n","    parser.add_argument(\"--openai_api\", type=bool, default=False, help=\"Whether to use OpenAI API\")\n","    parser.add_argument(\"--azure\", action=\"store_true\", default=False, help=\"Azure openai API\")\n","\n","    # Decoding\n","    parser.add_argument(\"--temperature\", type=float, default=0.5, help=\"Temperature for decoding\")\n","    parser.add_argument(\"--top_p\", type=float, default=1.0, help=\"Nucleus sampling top-p\")\n","    parser.add_argument(\"--max_new_tokens\", type=int, default=300, help=\"Max number of new tokens to generate in one step\")\n","    parser.add_argument(\"--max_length\", type=int, default=2048, help=\"Max length the model can take. Should set properly wrt the model to avoid position overflow.\")\n","    parser.add_argument(\"--num_samples\", type=int, default=1, help=\"Sample multiple answers.\")\n","\n","    # Use summarization/extraction of the documents\n","    parser.add_argument(\"--use_shorter\", type=str, default=None, help=\"Whether to use summary data or extraction data for documents. Option: None, `summary`, `extraction`\")\n","\n","    # Interactive\n","    parser.add_argument(\"--interactive\", type=bool, default=False, help=\"Whether to run in interactive mode\")\n","    parser.add_argument(\"--interactive_query\", type=str, default=None, help=\"The query to use in interactive mode, either `doc_id` (corresponding to interact in paper) or `search` (corresponding to inlinesearch in paper).\")\n","    parser.add_argument(\"--retriever\", type=str, default=None, help=\"When using interactive search mode, which retriever to use. Options: `tfidf`, `gtr-t5-large`\")\n","    parser.add_argument(\"--retriever_device\", type=str, default=\"cuda\", help=\"Where to put the dense retriever if using. Options: `cuda`, `cpu`\")\n","    parser.add_argument(\"--retrieve_in_all_docs\", type=bool, default=False, help=\"Retrieve in all documents instead of just top ndoc\")\n","    parser.add_argument(\"--max_turn\", type=int, default=10, help=\"Max number of all actions\")\n","    parser.add_argument(\"--max_doc_show\", type=int, default=3, help=\"Max number of documents to show at one time.\")\n","    parser.add_argument(\"--force_cite_show\", type=bool, default=False, help=\"Force citing the documents that are shown to the model\")\n","\n","\n","    # Load config\n","    args = parser.parse_args()\n","    config = yaml.safe_load(open(args.config)) if args.config is not None else {}\n","    parser.set_defaults(**config)\n","    args = parser.parse_args()\n","    for k in args.__dict__:\n","        print(f\"{k}: {args.__dict__[k]}\")\n","\n","    if \"turbo\" in args.model:\n","        # ChatGPT has a longer max length\n","        args.max_length = 4096\n","\n","    if \"16k\" in args.model:\n","        args.max_length = 16384\n","    elif \"32k\" in args.model:\n","        args.max_length = 32768\n","    elif \"turbo\" in args.model:\n","        args.max_length = 4096\n","    elif \"gpt-4\" in args.model:\n","        args.max_length = 8192\n","    elif \"llama-2\" in args.model.lower() or \"llama2\" in args.model.lower():\n","        args.max_length = 4096\n","\n","\n","    logger.info(f\"Set the model max length to {args.max_length} (if not correct, check the code)\")\n","\n","\n","    # Load the model or setup the API\n","    llm = LLM(args)\n","\n","    # Generate prompts\n","    np.random.seed(args.seed)\n","\n","    # Load data\n","    prompt_data = json.load(open(args.prompt_file))\n","    eval_data = json.load(open(args.eval_file))\n","\n","    # Generate the demonstration part\n","    head_prompt = \"\"\n","    train_ids = np.random.choice(len(prompt_data[\"demos\"]), args.shot, replace=False)\n","    for train_id in train_ids:\n","        train_item = prompt_data[\"demos\"][train_id]\n","        ndoc = args.ndoc\n","        if args.no_doc_in_demo:\n","            ndoc = 0\n","        elif args.fewer_doc_in_demo:\n","            assert args.ndoc_in_demo is not None\n","            ndoc = args.ndoc_in_demo\n","        head_prompt += make_demo(\n","            train_item, prompt=prompt_data[\"demo_prompt\"], ndoc=ndoc, doc_prompt=prompt_data[\"doc_prompt\"],\n","            instruction=prompt_data[\"instruction\"], use_shorter=args.use_shorter\n","        )\n","        head_prompt += prompt_data[\"demo_sep\"]\n","\n","    # Sample quick test\n","    if args.quick_test is not None:\n","        eval_ids = np.random.choice(len(eval_data), args.quick_test, replace=False)\n","        eval_data = [eval_data[int(idx)] for idx in eval_ids]\n","\n","    logger.info(\"Generating prompts...\")\n","    incomplete_doc_list = 0 # For some questions there might be fewer than ndoc documents\n","    for idx, eval_item in enumerate(tqdm(eval_data)):\n","        eval_data[idx]['prompt'] = head_prompt + make_demo(\n","            eval_item, prompt=prompt_data[\"demo_prompt\"], ndoc=args.ndoc, doc_prompt=prompt_data[\"doc_prompt\"],\n","            instruction=prompt_data[\"instruction\"], use_shorter=args.use_shorter,\n","            test=True\n","        )\n","        doc_list = get_shorter_text(eval_item, eval_item[\"docs\"], args.ndoc, args.use_shorter) if args.use_shorter is not None else eval_item[\"docs\"][:args.ndoc]\n","        if not args.retrieve_in_all_docs:\n","            # If --retrieve_in_all_docs, we keep the original docs and do not trim them by ndoc\n","            # Otherwise, take the new docs (truncated by ndoc and filtered if using summary/extraction)\n","            eval_data[idx]['docs'] = doc_list\n","        if len(doc_list) < args.ndoc:\n","            incomplete_doc_list += 1\n","    logger.info(\"Done.\")\n","    if incomplete_doc_list > 0:\n","        logger.warning(f\"There are {incomplete_doc_list} questions that have incomplete document list (may due to a lot of them are filtered out by summary/extraction).\")\n","\n","    # Load retriever for interactive search\n","    if args.interactive and args.interactive_query == \"search\" and \"gtr\" in args.retriever:\n","        from sentence_transformers import SentenceTransformer\n","        gtr_model = SentenceTransformer(f'sentence-transformers/{args.retriever}', device=args.retriever_device)\n","        from searcher import SearcherWithinDocs\n","\n","    for idx, item in enumerate(tqdm(eval_data)):\n","        prompt = item['prompt']\n","        prompt_len = len(llm.tokenizer.tokenize(prompt))\n","\n","        if idx == 0:\n","            print(prompt)\n","\n","        output_array = []\n","        for _ in range(args.num_samples):\n","            if args.interactive:\n","                print(\"============ Interactive =============\")\n","                output_answer = \"\"\n","                doc_list = item['docs']\n","\n","                interactive_prompt = prompt.rstrip() + \"\\n\" # Start a new line\n","                inline_doc = \"\"\n","                num_turn = 0\n","\n","                doc_history = []\n","                while True:\n","                    # For each action, it should end at the new line\n","                    # Three possible actions\n","                    # - Check: Document [1][2][3] / search query\n","                    # - Output: output\n","                    # - End\n","                    num_turn += 1\n","                    new_prompt = interactive_prompt + inline_doc\n","                    new_prompt_len = len(llm.tokenizer.tokenize(new_prompt))\n","\n","                    if idx == 0:\n","                        print(f\"-------------- Step {num_turn} prompt --------------\")\n","                        print(new_prompt)\n","                        print(\"-----------------------------\")\n","\n","                    output = llm.generate(new_prompt, min(args.max_new_tokens, args.max_length-new_prompt_len), stop=[\"\\n\", \"\\n\\n\"])\n","\n","                    if len(inline_doc) > 0:\n","                        output = \"Output: \" + output # \"Output: \" was included in inline_doc\n","                    inline_doc = \"\" # Delete inline_doc after use\n","                    interactive_prompt += output + \"\\n\"\n","                    logger.info(f\"Model output: \\\"{output}\\\"\")\n","\n","                    if output.strip().lower()[:3] == \"end\":\n","                        # Model decides to end the generation\n","                        break\n","                    elif \"sorry\" in output.lower() and (\"relevant document\" in output.lower() or \"relevant information\" in output.lower()) or \"none of the documents\" in output.lower():\n","                        # Instruction-tuned model may abstain from answer the question\n","                        break\n","                    elif output.strip().lower()[:5] == \"check\" or output.strip().lower()[:6] == \"search\":\n","                        # Checkout or search documents\n","                        if args.interactive_query == \"search\":\n","                            query = output.replace(\"Search:\", \"\").replace(\"search:\", \"\").strip()\n","                            if len(doc_list) == 0:\n","                                show_doc_ids = []\n","                            else:\n","                                searcher = SearcherWithinDocs(doc_list, args.retriever, model=gtr_model, device=args.retriever_device)\n","                                show_doc_ids = [int(searcher.search(query))]\n","                        elif args.interactive_query == \"doc_id\":\n","                            show_doc_ids = [int(r[1:])-1 for r in re.findall(r\"\\[\\d+\", output)] # In text citation id starts from 1\n","                            show_doc_ids = [doc_id for doc_id in show_doc_ids if doc_id < len(doc_list) and doc_id >= 0]\n","                            show_doc_ids = show_doc_ids[:args.max_doc_show] # Avoiding showing too many documents\n","                        else:\n","                            raise NotImplementedError\n","\n","                        inline_doc = \"\".join([make_doc_prompt(doc_list[doc_id], doc_id, prompt_data[\"doc_prompt\"]) for doc_id in show_doc_ids])\n","                        inline_doc += \"Output:\" # Force the model to generate output in the next step\n","                        doc_history.append(show_doc_ids)\n","                    elif output.strip().lower()[:6] == \"output\":\n","                        output = output.strip().replace(\"Output:\", \"\").strip()\n","                        if args.force_cite_show:\n","                            output = remove_citations(output)\n","                            if len(doc_history) == 0:\n","                                logger.warn(\"No doc history??\")\n","                            else:\n","                                # Just cite whatever documents the model has seen in the last step\n","                                if \"qampari\" in args.eval_file:\n","                                    output = \", \".join([\"\".join([f\"[{doc+1}]\" for doc in doc_history[-1]]) + \" \" + entity.strip() for entity in output.rstrip().rstrip(\",\").split(\",\")]) + \", \"\n","                                else:\n","                                    output = \" \".join([\"\".join([f\"[{doc+1}]\" for doc in doc_history[-1]]) + \" \" + o for o in sent_tokenize(output)]) + \".\"\n","                        output_answer += \" \" + output\n","                    else:\n","                        # Sometimes model starts to output random things.\n","                        break\n","\n","                    if num_turn >= args.max_turn:\n","                        logger.warning(\"Reach maximum number of turns. Terminate now.\")\n","                        break\n","\n","                if \"qampari\" in args.eval_file:\n","                    output_answer = output_answer.rstrip().rstrip(\",\")\n","                output_array.append(output_answer)\n","                item['prompt'] = interactive_prompt\n","                item['doc_history'] = doc_history\n","            else:\n","                output_array.append(llm.generate(prompt, min(args.max_new_tokens, args.max_length-prompt_len)))\n","                item['prompt'] = prompt\n","\n","            output_array[-1] = output_array[-1].replace(\"<|im_end|>\", \"\").rstrip()\n","            if output_array[-1].endswith(\"End.\"):\n","                output_array[-1] = output_array[-1][:-len(\"End.\")]\n","\n","            logger.info(f\"Prompt length={prompt_len}\")\n","            logger.info(f\"Question: {item['question']}\")\n","            logger.info(f\"Gold answer: {item['answer']}\")\n","            logger.info(f\"Final model output: {output_array[-1]}\")\n","\n","        item['output'] = output_array if len(output_array) > 1 else output_array[0]\n","\n","    logger.info(f\"#Cases when prompts exceed max length: {llm.prompt_exceed_max_length}\")\n","    logger.info(f\"#Cases when max new tokens < 50: {llm.fewer_than_50}\")\n","\n","    # Save the result\n","    model_name = args.model\n","    if \"/\" in model_name:\n","        model_name = model_name.split(\"/\")[-1]\n","    name = f\"{args.dataset_name}-{model_name}-{args.tag}-shot{args.shot}-ndoc{args.ndoc}-{args.seed}\"\n","    if args.azure:\n","        name += \"-azure\"\n","    if args.quick_test is not None:\n","        name += f\"-quick_test{args.quick_test}\"\n","    if args.no_doc_in_demo:\n","        name += \"-no_doc_in_demo\"\n","    if args.fewer_doc_in_demo:\n","        name += f\"-{args.ndoc_in_demo}_doc_in_demo\"\n","    if args.num_samples > 1:\n","        name += f\"-sample{args.num_samples}\"\n","    if args.force_cite_show:\n","        name += f\"-forceciteshow\"\n","\n","\n","    eval_data = {\n","        \"args\": args.__dict__,\n","        \"data\": eval_data,\n","    }\n","    if args.openai_api:\n","        logger.info(f\"Token used: prompt {llm.prompt_tokens}; completion {llm.completion_tokens}\")\n","        if \"turbo\" in args.model:\n","            p_price, c_price = 0.0015, 0.002\n","            if \"16k\" in args.model:\n","                p_price, c_price = 0.003, 0.004\n","        elif \"gpt4\" in args.model or \"gpt-4\" in args.model:\n","            p_price, c_price = 0.03, 0.06\n","            if \"32k\" in args.model:\n","                p_price, c_price = 0.06, 0.12\n","        else:\n","            logger.warn(\"Cannot find model price\")\n","            p_price, c_price = 0, 0\n","\n","        eval_data[\"total_cost\"] = llm.prompt_tokens / 1000 * p_price + llm.completion_tokens / 1000 * c_price\n","\n","        logger.info(f\"Unit price (Oct 16, 2023, prompt/completion): {p_price}/{c_price}\")\n","        logger.info(f\"Total cost: %.1f\" % (eval_data[\"total_cost\"]))\n","\n","        if args.azure:\n","            eval_data[\"azure_filter_fail\"] = llm.azure_filter_fail\n","\n","    if not os.path.exists(\"result\"):\n","        os.makedirs(\"result\")\n","    json.dump(eval_data, open(\"result/\" + name + \".json\", \"w\"), indent=4)\n","\n","if __name__ == \"__main__\":\n","    sys.argv = \"run.py --config configs/asqa_opt-6.7b_shot1_ndoc3_gtr_default.yaml\".split()\n","#     sys.argv = \"run.py --config configs/asqa_alpaca-7b_shot1_ndoc3_gtr_default.yaml\".split()\n","    main()"]},{"cell_type":"markdown","metadata":{"id":"f8wKxdAF8G5a"},"source":["# **eval.py**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tk-YLakk4gUm","trusted":true},"outputs":[],"source":["!pip install rouge-score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cqo5kODD9G6K","trusted":true},"outputs":[],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-15T15:05:37.992718Z","iopub.status.busy":"2024-08-15T15:05:37.992407Z","iopub.status.idle":"2024-08-15T15:05:40.076989Z","shell.execute_reply":"2024-08-15T15:05:40.075715Z","shell.execute_reply.started":"2024-08-15T15:05:37.992679Z"},"trusted":true},"outputs":[],"source":["!mkdir ./result\n","!cp /kaggle/input/secondidea/asqa-opt-6.7b-gtr-shot1-ndoc3-42.json ./result"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-15T15:13:18.918691Z","iopub.status.busy":"2024-08-15T15:13:18.918337Z","iopub.status.idle":"2024-08-15T16:06:13.505066Z","shell.execute_reply":"2024-08-15T16:06:13.504053Z","shell.execute_reply.started":"2024-08-15T15:13:18.918661Z"},"id":"a_gxs7Yd8NzR","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"733a3f4deca74b50a3b4f3eb8165a011","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"280690910b1441608841c4232019e4d7","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00001-of-00005.bin:  45%|####4     | 4.44G/9.86G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac8034b8453e4e11b250dd5cf095cea5","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00002-of-00005.bin:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d758c156e92b42519f61038e90d286d2","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00003-of-00005.bin:   0%|          | 0.00/9.93G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f3f7ac0bb18495a884888d5e50d1b17","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00004-of-00005.bin:   0%|          | 0.00/9.93G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f1e082d40ed45dc823e4442619c1c6f","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00005-of-00005.bin:   0%|          | 0.00/5.77G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72250bf118b045b5852eadeb1fe85d95","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9630cf1ccaa04f4f889333ff90052ea3","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c46faac191e48ee9da5192e6179d96d","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.42k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a59ca6cfe86435bb0d3032a4bf2c619","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2803e006862d41799e5f12181203f93e","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8640f822298473194bd71b3bf9d447f","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","  6%|▌         | 3/50 [00:30<08:00, 10.21s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n","100%|██████████| 50/50 [08:20<00:00, 10.01s/it]"]},{"name":"stdout","output_type":"stream","text":["Among all sentences, 2.87% have multiple citations, among which 33.33% are supported by the joint set, among which 150.00% overcite.\n","{'length': 87.7, 'str_em': 22.333333333333332, 'str_hit': 8.0, 'rougeLsum': 29.917325388689804, 'citation_rec': 4.2, 'citation_prec': 5.666666666666666}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import sys\n","import argparse\n","import collections\n","import json\n","import re\n","import string\n","import torch\n","import copy\n","\n","from nltk import sent_tokenize\n","import numpy as np\n","from rouge_score import rouge_scorer, scoring\n","from tqdm import tqdm\n","import sys\n","import logging\n","logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n","                    datefmt='%m/%d/%Y %H:%M:%S')\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","\n","from transformers import (\n","    AutoModelForSeq2SeqLM,\n","    AutoTokenizer,\n","    pipeline\n",")\n","\n","# from utils import normalize_answer, get_max_memory, remove_citations\n","\n","QA_MODEL=\"gaotianyu1350/roberta-large-squad\"\n","AUTOAIS_MODEL=\"google/t5_xxl_true_nli_mixture\"\n","\n","global autoais_model, autoais_tokenizer\n","autoais_model, autoais_tokenizer = None, None\n","\n","\n","def compute_f1(a_gold, a_pred):\n","    \"\"\"Compute F1 score between two strings.\"\"\"\n","\n","    def _get_tokens(s):\n","        if not s:\n","            return []\n","        return normalize_answer(s).split()\n","\n","    gold_toks = _get_tokens(a_gold)\n","    pred_toks = _get_tokens(a_pred)\n","\n","    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n","    num_same = sum(common.values())\n","\n","    if len(gold_toks) == 0 or len(pred_toks) == 0:\n","        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n","        return int(gold_toks == pred_toks)\n","\n","    if num_same == 0:\n","        return 0\n","\n","    precision = 1.0 * num_same / len(pred_toks)\n","    recall = 1.0 * num_same / len(gold_toks)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","\n","    return f1\n","\n","\n","def compute_exact(a_gold, a_pred):\n","    \"\"\"Check whether two strings are equal up to normalization.\"\"\"\n","\n","    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n","\n","\n","def exact_presence(short_answers, context):\n","    \"\"\"Verify if any of the answers is present in the given context.\n","    Args:\n","        short_answers: list of short answers to look for in the context\n","        context: a paragraph to search for short answers\n","    Returns:\n","        true if any of the short answers is present in the context\n","    \"\"\"\n","\n","    n_short_answers = [normalize_answer(sa) for sa in short_answers]\n","    n_context = normalize_answer(context)\n","\n","    for ans in n_short_answers:\n","        if ans in n_context:\n","            return True\n","\n","    return False\n","\n","\n","def compute_rouge(data):\n","    \"\"\"Main function for rouge scoring.\n","    If two references are provided,\n","    the best score is chosen for each instance.\n","    Args:\n","        data: requires field `output` and `answer` (or `annotations` for ASQA)\n","        metrics: list of evaluation metrics\n","    Returns:\n","        dictionary representation of rouge scores\n","    \"\"\"\n","    def _rouge_calculation(hypotheses,\n","                        references1,\n","                        references2=[],\n","                        metrics=['rougeLsum']):\n","\n","        if references2 == []:\n","            references2 = references1\n","\n","        scorer = rouge_scorer.RougeScorer(metrics, use_stemmer=True)\n","        aggregator = scoring.BootstrapAggregator()\n","\n","        for i in range(len(hypotheses)):\n","            scores1 = scorer.score(references1[i], hypotheses[i])\n","            scores2 = scorer.score(references2[i], hypotheses[i])\n","            if scores1['rougeLsum'].fmeasure > scores2['rougeLsum'].fmeasure:\n","                aggregator.add_scores(scores1)\n","            else:\n","                aggregator.add_scores(scores2)\n","\n","        scores = {m: [] for m in metrics}\n","\n","        for m in metrics:\n","            fmeasure = aggregator.aggregate()[m].mid.fmeasure\n","            scores[m].append(fmeasure)\n","\n","        for m in scores:\n","            scores[m] = 100 * sum(scores[m]) / len(scores[m])\n","\n","        return scores\n","\n","    hypotheses = {}\n","    references1 = {}\n","    references2 = {}\n","\n","    for idx, item in enumerate(data):\n","        hypotheses[idx] = item[\"output\"]\n","        if \"annotations\" in item and item['annotations'] is not None: # For ASQA\n","            references1[idx] = item[\"annotations\"][0][\"long_answer\"]\n","            references2[idx] = item[\"annotations\"][1][\"long_answer\"]\n","        else:\n","            references1[idx] = item[\"answer\"]\n","            references2[idx] = item[\"answer\"]\n","\n","    h, r1, r2 = [], [], []\n","\n","    for key in references1:\n","        h.append(hypotheses[key])\n","        r1.append(references1[key])\n","\n","        if references2 is not None:\n","            r2.append(references2[key])\n","\n","    h = ['\\n'.join(sent_tokenize(text.lower())) for text in h]\n","    r1 = ['\\n'.join(sent_tokenize(text.lower())) for text in r1]\n","    r2 = ['\\n'.join(sent_tokenize(text.lower())) for text in r2]\n","    scores = _rouge_calculation(h, r1, r2)\n","\n","    return scores['rougeLsum']\n","\n","\n","def compute_str_em(data):\n","    \"\"\"Compute STR-EM metric (only for ASQA)\n","    Args:\n","        data: requires field `qa_pairs/short_answers` and `output`\n","    Returns:\n","        STR-EM and STR-EM-HIT ()\n","    \"\"\"\n","\n","    if 'qa_pairs' not in data[0] or data[0]['qa_pairs'] is None:\n","        return 0, 0\n","\n","    acc = []\n","    hit = []\n","\n","    for item in data:\n","        loc_acc = []\n","        for qa_pair in item['qa_pairs']:\n","            loc_acc.append(exact_presence(qa_pair['short_answers'], item[\"output\"]))\n","        acc.append(np.mean(loc_acc))\n","        hit.append( int(np.mean(loc_acc) == 1) )\n","\n","    return 100 * np.mean(acc), 100 * np.mean(hit)\n","\n","\n","def compute_len(data):\n","    \"\"\"Compute average length of predictions.\"\"\"\n","\n","    res, cntr = 0, 0\n","    for item in data:\n","        res += len(item[\"output\"].split())\n","        cntr += 1\n","    return res / cntr\n","\n","\n","def compute_qa(data):\n","    \"\"\"Compute QA-based accuracy.\n","    Args:\n","        data: requires filed `qa_pairs/short_answers` and `output`\n","    Returns:\n","        QA metrics (QA-EM, QA-F1, QA-Hit)\n","    \"\"\"\n","\n","    if 'qa_pairs' not in data[0] or data[0]['qa_pairs'] is None:\n","        logger.warn(\"Warning: no QA pairs found in data\")\n","        return {\n","            'QA-EM': 0,\n","            'QA-F1': 0,\n","            'QA-Hit': 0,\n","        }\n","\n","    # Load model\n","    logger.info(\"Loading the RoBERTa-large SQuAD model for QA-based accuracy...\")\n","    qa_pipeline = pipeline(\"question-answering\", model=QA_MODEL, device=0)\n","    logger.info(\"Done\")\n","\n","    # Get prediction\n","    logger.info(\"Computing the QA-based accuracy...\")\n","    em, f1, bins = [], [], []\n","    for item in tqdm(data):\n","        question = [qa_pair['question'] for qa_pair in item['qa_pairs']]\n","        context = item['output'] if len(item['output']) > 0 else \" \"\n","        results = qa_pipeline(question=question, context=context, handle_impossible_answer=True)\n","        loc_counter, loc_em, loc_f1 = 0, 0, 0\n","\n","        for idx, res in enumerate(results):\n","            answers = item[\"qa_pairs\"][idx][\"short_answers\"]\n","            prediction = res[\"answer\"]\n","\n","            loc_em += max([compute_exact(a, prediction) for a in answers])\n","            loc_f1 += max([compute_f1(a, prediction) for a in answers])\n","            loc_counter += 1\n","\n","        em.append(loc_em / loc_counter)\n","        f1.append(loc_f1 / loc_counter)\n","        bins.append(loc_em == loc_counter)\n","\n","    return {\n","        'QA-EM': 100 * np.mean(em),\n","        'QA-F1': 100 * np.mean(f1),\n","        'QA-Hit': 100 * np.mean(bins)\n","    }\n","\n","\n","def compute_mauve(data):\n","    \"\"\"Compute Mauve score.\"\"\"\n","\n","    logger.info(\"Computing MAUVE...\")\n","    human_data = []\n","    model_data = []\n","    for item in data:\n","        # Remove ending punctuations\n","        # Remove any new lines\n","        # Truncate by 100 words\n","        human_data.append(' '.join((item['question'] + \" \" + item['answer'].strip()).split()[:100]).rstrip(string.punctuation))\n","        model_data.append(' '.join((item['question'] + \" \" + item['output'].strip()).split()[:100]).rstrip(string.punctuation))\n","\n","    import mauve\n","    out = mauve.compute_mauve(\n","        p_text=human_data,\n","        q_text=model_data,\n","        device_id=0,\n","        max_text_length=512,\n","        verbose=True,\n","        batch_size=8,\n","        featurize_model_name=\"gpt2-large\"\n","    )\n","    return out.mauve * 100\n","\n","\n","def _run_nli_autoais(passage, claim):\n","    \"\"\"\n","    Run inference for assessing AIS between a premise and hypothesis.\n","    Adapted from https://github.com/google-research-datasets/Attributed-QA/blob/main/evaluation.py\n","    \"\"\"\n","    global autoais_model, autoais_tokenizer\n","    input_text = \"premise: {} hypothesis: {}\".format(passage, claim)\n","    input_ids = autoais_tokenizer(input_text, return_tensors=\"pt\").input_ids.to(autoais_model.device)\n","    with torch.inference_mode():\n","        outputs = autoais_model.generate(input_ids, max_new_tokens=10)\n","    result = autoais_tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    inference = 1 if result == \"1\" else 0\n","    return inference\n","\n","\n","def compute_claims(data):\n","    global autoais_model, autoais_tokenizer\n","    if autoais_model is None:\n","        logger.info(\"Loading AutoAIS model...\")\n","        autoais_model = AutoModelForSeq2SeqLM.from_pretrained(AUTOAIS_MODEL, torch_dtype=torch.bfloat16, max_memory=get_max_memory(), device_map=\"auto\")\n","        autoais_tokenizer = AutoTokenizer.from_pretrained(AUTOAIS_MODEL, use_fast=False)\n","\n","    logger.info(\"Computing claims...\")\n","    scores = []\n","    for item in tqdm(data):\n","        normalized_output = remove_citations(item['output'])\n","        entail = 0\n","        claims = item[\"claims\"]\n","        for claim in claims:\n","            entail += _run_nli_autoais(normalized_output, claim)\n","        scores.append(entail / len(claims))\n","    return 100 * np.mean(scores)\n","\n","\n","def compute_autoais(data,\n","                    decontext=False,\n","                    concat=False,\n","                    qampari=False,\n","                    at_most_citations=None,):\n","    \"\"\"\n","    Compute AutoAIS score.\n","\n","    Args:\n","        data: requires field `output` and `docs`\n","              - docs should be a list of items with fields `title` and `text` (or `phrase` and `sent` for QA-extracted docs)\n","        citation: check citations and use the corresponding references.\n","        decontext: decontextualize the output\n","    \"\"\"\n","\n","    global autoais_model, autoais_tokenizer\n","    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # Added !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","    if autoais_model is None:\n","        logger.info(\"Loading AutoAIS model...\")\n","        autoais_model = AutoModelForSeq2SeqLM.from_pretrained(\n","            AUTOAIS_MODEL,\n","            torch_dtype=torch.bfloat16,\n","            max_memory=get_max_memory(),\n","            offload_folder='offload_folder2', # Added !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","            device_map=\"auto\",\n","#             device_map=device,\n","        )\n","        autoais_tokenizer = AutoTokenizer.from_pretrained(AUTOAIS_MODEL, use_fast=False)\n","\n","    logger.info(f\"Running AutoAIS...\")\n","\n","    def _format_document(doc):\n","        \"\"\"Format document for AutoAIS.\"\"\"\n","\n","        if \"sent\" in doc:\n","            # QA-extracted docs\n","            return \"Title: %s\\n%s\" % (doc['title'], doc['sent'])\n","        else:\n","            return \"Title: %s\\n%s\" % (doc['title'], doc['text'])\n","\n","    ais_scores = []\n","    ais_scores_prec = []\n","\n","    sent_total = 0\n","    sent_mcite = 0\n","    sent_mcite_support = 0\n","    sent_mcite_overcite = 0\n","    autoais_log = []\n","    for item in tqdm(data):\n","        # Get sentences by using NLTK\n","        if qampari:\n","            sents = [item['question'] + \" \" + x.strip() for x in item['output'].rstrip().rstrip(\".\").rstrip(\",\").split(\",\")]\n","        else:\n","            sents = sent_tokenize(item['output'])\n","        if len(sents) == 0:\n","            continue\n","\n","        target_sents = [remove_citations(sent).strip() for sent in sents]\n","\n","        entail = 0\n","        entail_prec = 0\n","        total_citations = 0\n","        for sent_id, sent in enumerate(sents):\n","            target_sent = target_sents[sent_id] # Citation removed and (if opted for) decontextualized\n","            joint_entail = -1 # Undecided\n","\n","            # Find references\n","            ref = [int(r[1:])-1 for r in re.findall(r\"\\[\\d+\", sent)] # In text citation id starts from 1\n","            logger.info(f\"For `{sent}`, find citations {ref}\")\n","            if len(ref) == 0:\n","                # No citations\n","                joint_entail = 0\n","            elif any([ref_id >= len(item['docs']) for ref_id in ref]):\n","                # Citations out of range\n","                joint_entail = 0\n","            else:\n","                if at_most_citations is not None:\n","                    ref = ref[:at_most_citations]\n","                total_citations += len(ref)\n","                joint_passage = '\\n'.join([_format_document(item['docs'][psgs_id]) for psgs_id in ref])\n","\n","            # If not directly rejected by citation format error, calculate the recall score\n","            if joint_entail == -1:\n","                joint_entail = _run_nli_autoais(joint_passage, target_sent)\n","                autoais_log.append({\n","                    \"question\": item['question'],\n","                    \"output\": item['output'],\n","                    \"claim\": sent,\n","                    \"passage\": [joint_passage],\n","                    \"model_type\": \"NLI\",\n","                    \"model_output\": joint_entail,\n","                })\n","\n","            entail += joint_entail\n","            if len(ref) > 1:\n","                sent_mcite += 1\n","\n","            # calculate the precision score if applicable\n","            if joint_entail and len(ref) > 1:\n","                sent_mcite_support += 1\n","                # Precision check: did the model cite any unnecessary documents?\n","                for psgs_id in ref:\n","                    # condition A\n","                    passage = _format_document(item['docs'][psgs_id])\n","                    nli_result = _run_nli_autoais(passage, target_sent)\n","\n","                    # condition B\n","                    if not nli_result:\n","                        subset_exclude = copy.deepcopy(ref)\n","                        subset_exclude.remove(psgs_id)\n","                        passage = '\\n'.join([_format_document(item['docs'][pid]) for pid in subset_exclude])\n","                        nli_result = _run_nli_autoais(passage, target_sent)\n","                        if nli_result: # psgs_id is not necessary\n","                            flag = 0\n","                            sent_mcite_overcite += 1\n","                        else:\n","                            entail_prec += 1\n","                    else:\n","                        entail_prec += 1\n","            else:\n","                entail_prec += joint_entail\n","\n","        sent_total += len(sents)\n","        ais_scores.append(entail / len(sents))\n","        ais_scores_prec.append(entail_prec / total_citations if total_citations > 0 else 0) # len(sents))\n","\n","    if sent_mcite > 0 and sent_mcite_support > 0:\n","        print(\"Among all sentences, %.2f%% have multiple citations, among which %.2f%% are supported by the joint set, among which %.2f%% overcite.\" % (\n","            100 * sent_mcite / sent_total,\n","            100 * sent_mcite_support / sent_mcite,\n","            100 * sent_mcite_overcite / sent_mcite_support\n","        ))\n","\n","    return {\n","        \"citation_rec\": 100 * np.mean(ais_scores),\n","        \"citation_prec\": 100 * np.mean(ais_scores_prec),\n","    }\n","\n","\n","def compute_qampari_f1(data, cot=False):\n","    prec = []\n","    rec = []\n","    rec_top5 = []\n","    f1 = []\n","    f1_top5 = []\n","\n","    num_preds = []\n","    for item in data:\n","        if cot:\n","            if \":\" in item['output']:\n","                o = ':'.join(item['output'].split(\":\")[1:]) # try to separate the COT part and the answer list part.\n","            else:\n","                o = \"\"\n","        else:\n","            o = item['output']\n","        preds = [normalize_answer(x.strip()) for x in o.rstrip().rstrip(\".\").rstrip(\",\").split(\",\")]\n","        preds = [p for p in preds if len(p) > 0] # delete empty answers\n","        num_preds.append(len(preds))\n","        answers = [[normalize_answer(x) for x in ans] for ans in item['answers']]\n","        flat_answers = [item for sublist in answers for item in sublist]\n","\n","        prec.append(sum([p in flat_answers for p in preds]) / len(preds) if len(preds) > 0 else 0)\n","        rec.append(sum([any([x in preds for x in a]) for a in answers]) / len(answers))\n","        rec_top5.append(min(5, sum([any([x in preds for x in a]) for a in answers])) / min(5, len(answers)))\n","        if (prec[-1] + rec[-1]) == 0:\n","            f1.append(0)\n","        else:\n","            f1.append(2 * prec[-1] * rec[-1] / (prec[-1] + rec[-1]))\n","        if (prec[-1] + rec_top5[-1]) == 0:\n","            f1_top5.append(0)\n","        else:\n","            f1_top5.append(2 * prec[-1] * rec_top5[-1] / (prec[-1] + rec_top5[-1]))\n","\n","    return {\n","        \"num_preds\": np.mean(num_preds),\n","        \"qampari_prec\": 100 * np.mean(prec),\n","        \"qampari_rec\": 100 * np.mean(rec),\n","        \"qampari_rec_top5\": 100 * np.mean(rec_top5),\n","        \"qampari_f1\": 100 * np.mean(f1),\n","        \"qampari_f1_top5\": 100 * np.mean(f1_top5),\n","    }\n","\n","def main():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--f\", type=str, required=True, help=\"Output file. Should have field `question`, `output`, (ROUGE) `answer`, \\\n","                        (accuracy) `qa_pairs`, (AIS) `docs`\")\n","    parser.add_argument(\"--no_rouge\", action=\"store_true\", help=\"Do not evaluate ROUGE score\")\n","    parser.add_argument(\"--qa\", action=\"store_true\", help=\"Use the QA model\")\n","    parser.add_argument(\"--mauve\", action=\"store_true\", help=\"Use the mauve score model\")\n","    parser.add_argument(\"--citations\", action=\"store_true\", help=\"Evaluation with citation\")\n","    parser.add_argument(\"--at_most_citations\", type=int, default=3, help=\"At most take this many documents (mostly for precision)\")\n","    parser.add_argument(\"--claims_nli\", action=\"store_true\", help=\"Use claims for ELI5\")\n","\n","    # QAMPARI\n","    parser.add_argument(\"--cot\", action=\"store_true\", help=\"For QAMPARI, try to find colon and separate the COT and answer listing\")\n","\n","    args = parser.parse_args()\n","\n","    with open(args.f) as f:\n","        data_with_config = json.load(f)\n","    data = data_with_config['data']\n","\n","    if \"qampari\" in args.f:\n","        args.no_rouge = True\n","        args.qa = False\n","        args.mauve = False\n","        args.decontext = False\n","        qampari = True\n","    else:\n","        qampari = False\n","\n","    # Truncate by newline and remove on the fly search result\n","    logger.warning(\"We remove all the pre/appended space/newlines and we truncate the answer by the first newline.\")\n","    logger.warning(\"We replace any on the fly search result to standard bracket citation format.\")\n","    for i in range(len(data)):\n","        data[i]['output'] = data[i]['output'].strip().split(\"\\n\")[0]\n","        data[i]['output'] = data[i]['output'].replace(\"<|im_end|>\", \"\")\n","\n","\n","    # Remove all citations for all non-AutoAIS evaluation\n","    normalized_data = copy.deepcopy(data)\n","    for i in range(len(normalized_data)):\n","        normalized_data[i]['output'] = remove_citations(normalized_data[i]['output'])\n","\n","    result = {}\n","    result['length'] = compute_len(normalized_data)\n","    result['str_em'], result['str_hit'] = compute_str_em(normalized_data)\n","    if qampari:\n","        result.update(compute_qampari_f1(normalized_data, cot=args.cot))\n","    if not args.no_rouge:\n","        result['rougeLsum'] = compute_rouge(normalized_data)\n","    if args.qa:\n","        result.update(compute_qa(normalized_data))\n","    if args.mauve:\n","        result['mauve'] = compute_mauve(normalized_data)\n","    if args.citations:\n","        result.update(compute_autoais(data, qampari=qampari, at_most_citations=args.at_most_citations))\n","    if args.claims_nli:\n","        result[\"claims_nli\"] = compute_claims(normalized_data)\n","\n","    print(result)\n","    with open(args.f + \".score\", \"w\") as f:\n","        json.dump(result, f, indent=4)\n","\n","\n","if __name__ == \"__main__\":\n","    sys.argv = \"eval.py --f ./result/asqa-opt-6.7b-gtr-shot1-ndoc3-42.json --citations\".split()\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDyXHSR_9KKl"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["C4CYHeXJvygY","MwY33ndowL8K","0lqYT7CKwY8w","OIb6oljrwsQZ","a5o9EwScwyHA"],"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5548510,"sourceId":9180116,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1263bbd624f44d168e9e70d1371e8b26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd3b87406fcc496b82ecefab683efeb6","placeholder":"​","style":"IPY_MODEL_b4876cd4a8af46119941b763bd5738d0","value":"Loading checkpoint shards: 100%"}},"1ac81e0f1bef40dab07f6c7e6086e590":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"241504e2b1da4c9297b4ee6ed37f9fe8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6eb11b65f29b4a5e9cb175f9916e5b12":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d3a59579ed942f7a1c47998a1eb7db3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4876cd4a8af46119941b763bd5738d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb0cf9cf297945adb1fbe1e4b92d6987":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ac81e0f1bef40dab07f6c7e6086e590","placeholder":"​","style":"IPY_MODEL_d521c60c368546578607378ed6073b37","value":" 2/2 [00:53&lt;00:00, 24.59s/it]"}},"cd3b87406fcc496b82ecefab683efeb6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d521c60c368546578607378ed6073b37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3c70a1c828441eeb26763c7dd26ef5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d3a59579ed942f7a1c47998a1eb7db3","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_241504e2b1da4c9297b4ee6ed37f9fe8","value":2}},"eedc5afd201345229a9ee82ff8bc30ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1263bbd624f44d168e9e70d1371e8b26","IPY_MODEL_e3c70a1c828441eeb26763c7dd26ef5f","IPY_MODEL_cb0cf9cf297945adb1fbe1e4b92d6987"],"layout":"IPY_MODEL_6eb11b65f29b4a5e9cb175f9916e5b12"}}}}},"nbformat":4,"nbformat_minor":4}
