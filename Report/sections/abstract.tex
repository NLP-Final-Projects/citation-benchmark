\textit{In this paper, we propose a comprehensive approach to enhance the evaluation of language models by focusing on the generation and utilization of reference citations. The evaluation of model outputs is crucial in determining their quality and relevance, yet current methodologies often rely heavily on human judgment, which can be time-consuming and unreliable. To address this issue, we have developed a standardized benchmark framework, including novel metrics that emphasize automated, correlation-based assessments of sentence fragments within documents. This framework includes optimized input processing techniques, structured input formats, and improved methods for extracting references from web pages. Leveraging diverse datasets such as ASQA, QAMPARI, ELI5, and Wikidata5m, our approach aims to create a robust and scalable evaluation system. By employing models such as Vicuna and LLaMA alongside established models like GPT-4, we study the effectiveness of these strategies in reducing human intervention and enhancing the accuracy of model evaluation.}